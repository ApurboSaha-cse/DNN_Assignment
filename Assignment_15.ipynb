{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Building BERT Model from Scratch"
      ],
      "metadata": {
        "id": "QNLfLZFnnrQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Configuration for BERT model\n",
        "@dataclass\n",
        "class BERTConfig:\n",
        "    vocab_size: int = 30000\n",
        "    max_len: int = 128\n",
        "    embed_dim: int = 256  # Reduced for faster training\n",
        "    num_heads: int = 8\n",
        "    ff_dim: int = 512\n",
        "    num_encoder_blocks: int = 4  # Reduced for faster training\n",
        "    dropout_rate: float = 0.1\n",
        "    mlm_probability: float = 0.15\n",
        "\n",
        "config = BERTConfig()\n",
        "\n",
        "# ============ BERT Components ============\n",
        "\n",
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        assert embed_dim % num_heads == 0\n",
        "\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "\n",
        "        query = self.separate_heads(query, batch_size)\n",
        "        key = self.separate_heads(key, batch_size)\n",
        "        value = self.separate_heads(value, batch_size)\n",
        "\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
        "        output = self.combine_heads(concat_attention)\n",
        "        return output\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"gelu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "# ============ BERT Model ============\n",
        "\n",
        "def create_bert_model(config, mlm=True):\n",
        "    inputs = layers.Input(shape=(config.max_len,), dtype=tf.int32)\n",
        "\n",
        "    # Embedding layer\n",
        "    embedding_layer = TokenAndPositionEmbedding(\n",
        "        config.max_len, config.vocab_size, config.embed_dim\n",
        "    )\n",
        "    x = embedding_layer(inputs)\n",
        "\n",
        "    # Transformer blocks\n",
        "    for _ in range(config.num_encoder_blocks):\n",
        "        x = TransformerBlock(\n",
        "            config.embed_dim, config.num_heads, config.ff_dim, config.dropout_rate\n",
        "        )(x, training=True)\n",
        "\n",
        "    if mlm:\n",
        "        # MLM head\n",
        "        mlm_output = layers.Dense(config.vocab_size, activation=\"softmax\", name=\"mlm_output\")(x)\n",
        "        model = keras.Model(inputs=inputs, outputs=mlm_output)\n",
        "    else:\n",
        "        model = keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============ Data Preparation for MLM ============\n",
        "\n",
        "class MLMDataGenerator:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.mask_token_id = 1  # [MASK]\n",
        "        self.pad_token_id = 0   # [PAD]\n",
        "\n",
        "    def encode_text(self, text, tokenizer):\n",
        "        \"\"\"Simple word-level tokenization\"\"\"\n",
        "        tokens = text.lower().split()[:self.config.max_len - 2]\n",
        "        token_ids = [tokenizer.get(token, 2) for token in tokens]  # 2 for [UNK]\n",
        "\n",
        "        # Padding\n",
        "        if len(token_ids) < self.config.max_len:\n",
        "            token_ids = token_ids + [self.pad_token_id] * (self.config.max_len - len(token_ids))\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "    def mask_tokens(self, inputs):\n",
        "        \"\"\"Create MLM targets by masking random tokens\"\"\"\n",
        "        labels = np.copy(inputs)\n",
        "\n",
        "        # Create random mask\n",
        "        probability_matrix = np.random.rand(*inputs.shape)\n",
        "        mask_arr = (probability_matrix < self.config.mlm_probability) & (inputs != self.pad_token_id)\n",
        "\n",
        "        # Replace masked tokens with [MASK] token\n",
        "        inputs[mask_arr] = self.mask_token_id\n",
        "\n",
        "        # Only compute loss on masked tokens (-100 is ignored in loss)\n",
        "        labels[~mask_arr] = -100\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "# ============ Sample Training Code ============\n",
        "\n",
        "def prepare_sample_data():\n",
        "    \"\"\"Prepare sample text data for demonstration\"\"\"\n",
        "    sample_texts = [\n",
        "        \"The quick brown fox jumps over the lazy dog\",\n",
        "        \"Machine learning is a subset of artificial intelligence\",\n",
        "        \"Natural language processing helps computers understand human language\",\n",
        "        \"Deep learning models require large amounts of data\",\n",
        "        \"Transformers have revolutionized natural language processing\",\n",
        "        \"BERT stands for bidirectional encoder representations from transformers\",\n",
        "        \"Attention mechanism allows models to focus on relevant parts of input\",\n",
        "        \"Pre-training and fine-tuning is a powerful paradigm in NLP\",\n",
        "    ] * 100  # Repeat for more training samples\n",
        "\n",
        "    # Build simple vocabulary\n",
        "    all_words = set()\n",
        "    for text in sample_texts:\n",
        "        all_words.update(text.lower().split())\n",
        "\n",
        "    vocab = {word: idx + 3 for idx, word in enumerate(sorted(all_words))}\n",
        "    vocab['[PAD]'] = 0\n",
        "    vocab['[MASK]'] = 1\n",
        "    vocab['[UNK]'] = 2\n",
        "\n",
        "    return sample_texts, vocab\n",
        "\n",
        "def train_mlm_model():\n",
        "    \"\"\"Train BERT with MLM objective\"\"\"\n",
        "    # Prepare data\n",
        "    texts, vocab = prepare_sample_data()\n",
        "    config.vocab_size = len(vocab)\n",
        "\n",
        "    # Create model\n",
        "    model = create_bert_model(config, mlm=True)\n",
        "\n",
        "    # Custom loss to ignore -100 labels\n",
        "    def masked_sparse_categorical_crossentropy(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        mask = tf.cast(y_true != -100, tf.float32)\n",
        "        y_true = tf.where(y_true == -100, 0, y_true)\n",
        "\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "        loss = loss * mask\n",
        "        return tf.reduce_sum(loss) / (tf.reduce_sum(mask) + 1e-10)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss=masked_sparse_categorical_crossentropy,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Prepare training data\n",
        "    data_gen = MLMDataGenerator(config)\n",
        "\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "\n",
        "    for text in texts:\n",
        "        token_ids = data_gen.encode_text(text, vocab)\n",
        "        token_ids = np.array(token_ids)\n",
        "        masked_input, labels = data_gen.mask_tokens(token_ids.copy())\n",
        "        X_train.append(masked_input)\n",
        "        y_train.append(labels)\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    print(f\"Training data shape: {X_train.shape}\")\n",
        "    print(f\"Model summary:\")\n",
        "    model.summary()\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        batch_size=32,\n",
        "        epochs=10,\n",
        "        validation_split=0.1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model, vocab, history\n",
        "\n",
        "# ============ Main Execution ============\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 50)\n",
        "    print(\"BERT from Scratch - MLM Training\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Train the model\n",
        "    bert_model, vocabulary, training_history = train_mlm_model()\n",
        "\n",
        "    # Save the model\n",
        "    bert_model.save('bert_mlm_model.h5')\n",
        "    print(\"\\nModel saved as 'bert_mlm_model.h5'\")\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "    print(f\"Final training loss: {training_history.history['loss'][-1]:.4f}\")\n",
        "    print(f\"Final validation loss: {training_history.history['val_loss'][-1]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KZ7iGobOnv5J",
        "outputId": "74dbd591-d696-4613-daa4-44916d94261b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "BERT from Scratch - MLM Training\n",
            "==================================================\n",
            "Training data shape: (800, 128)\n",
            "Model summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ token_and_position_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │        \u001b[38;5;34m47,616\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m527,104\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m527,104\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m527,104\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m527,104\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mlm_output (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m58\u001b[0m)        │        \u001b[38;5;34m14,906\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ token_and_position_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">47,616</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mlm_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">14,906</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,170,938\u001b[0m (8.28 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,170,938</span> (8.28 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,170,938\u001b[0m (8.28 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,170,938</span> (8.28 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 664ms/step - accuracy: 3.2266e-04 - loss: 4.3323 - val_accuracy: 4.8828e-04 - val_loss: 3.9538\n",
            "Epoch 2/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 7.4125e-04 - loss: 3.7717 - val_accuracy: 0.0027 - val_loss: 2.9297\n",
            "Epoch 3/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.0034 - loss: 2.3545 - val_accuracy: 0.0093 - val_loss: 1.0942\n",
            "Epoch 4/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.0082 - loss: 0.8990 - val_accuracy: 0.0111 - val_loss: 0.3275\n",
            "Epoch 5/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.0099 - loss: 0.2928 - val_accuracy: 0.0113 - val_loss: 0.1168\n",
            "Epoch 6/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.0092 - loss: 0.1345 - val_accuracy: 0.0114 - val_loss: 0.0670\n",
            "Epoch 7/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.0096 - loss: 0.0827 - val_accuracy: 0.0114 - val_loss: 0.0495\n",
            "Epoch 8/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.0099 - loss: 0.0627 - val_accuracy: 0.0114 - val_loss: 0.0354\n",
            "Epoch 9/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.0095 - loss: 0.0475 - val_accuracy: 0.0114 - val_loss: 0.0275\n",
            "Epoch 10/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.0091 - loss: 0.0326 - val_accuracy: 0.0114 - val_loss: 0.0243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model saved as 'bert_mlm_model.h5'\n",
            "\n",
            "Training completed!\n",
            "Final training loss: 0.0344\n",
            "Final validation loss: 0.0243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT finetuning - sentiment classification"
      ],
      "metadata": {
        "id": "XbZyldTUoY8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load pre-trained BERT model from MLM task\n",
        "# Assuming you have the BERT model architecture from Task 1\n",
        "\n",
        "def create_sentiment_classifier(bert_model, config, num_classes=3):\n",
        "    \"\"\"\n",
        "    Fine-tune BERT for sentiment classification\n",
        "    num_classes: 2 for binary (positive/negative), 3 for (positive/neutral/negative)\n",
        "    \"\"\"\n",
        "    # Freeze BERT layers initially (optional - can unfreeze for full fine-tuning)\n",
        "    for layer in bert_model.layers:\n",
        "        layer.trainable = True  # Set to False for feature extraction only\n",
        "\n",
        "    # Get BERT outputs\n",
        "    inputs = bert_model.input\n",
        "    bert_output = bert_model.output\n",
        "\n",
        "    # Take [CLS] token representation (first token)\n",
        "    cls_token = bert_output[:, 0, :]\n",
        "\n",
        "    # Classification head\n",
        "    x = layers.Dropout(0.3)(cls_token)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax', name='sentiment_output')(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# ============ Data Preparation ============\n",
        "\n",
        "def load_imdb_dataset():\n",
        "    \"\"\"Load IMDB movie review dataset for sentiment classification\"\"\"\n",
        "    from tensorflow.keras.datasets import imdb\n",
        "\n",
        "    # Load data\n",
        "    max_features = 10000\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "    # Decode reviews\n",
        "    word_index = imdb.get_word_index()\n",
        "    reverse_word_index = {v: k for k, v in word_index.items()}\n",
        "\n",
        "    def decode_review(encoded_review):\n",
        "        return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])\n",
        "\n",
        "    # Convert to text\n",
        "    x_train_text = [decode_review(x) for x in x_train[:5000]]  # Use subset for demo\n",
        "    x_test_text = [decode_review(x) for x in x_test[:1000]]\n",
        "    y_train = y_train[:5000]\n",
        "    y_test = y_test[:1000]\n",
        "\n",
        "    return x_train_text, y_train, x_test_text, y_test\n",
        "\n",
        "def prepare_sentiment_data_simple():\n",
        "    \"\"\"Prepare simple sentiment dataset for demonstration\"\"\"\n",
        "    # Positive reviews\n",
        "    positive_texts = [\n",
        "        \"This movie is absolutely fantastic and amazing\",\n",
        "        \"I loved every moment of this film\",\n",
        "        \"Outstanding performance by all actors\",\n",
        "        \"A masterpiece of cinema\",\n",
        "        \"Highly recommended must watch film\",\n",
        "        \"Brilliant storytelling and great direction\",\n",
        "        \"Best movie I have seen this year\",\n",
        "        \"Absolutely wonderful experience\",\n",
        "        \"Exceeded all my expectations\",\n",
        "        \"Perfect blend of action and emotion\",\n",
        "    ] * 50\n",
        "\n",
        "    # Negative reviews\n",
        "    negative_texts = [\n",
        "        \"Terrible movie waste of time\",\n",
        "        \"Very disappointing and boring\",\n",
        "        \"Worst film ever made\",\n",
        "        \"Complete disaster terrible acting\",\n",
        "        \"Do not watch this movie\",\n",
        "        \"Awful storyline and poor direction\",\n",
        "        \"Boring and predictable plot\",\n",
        "        \"Waste of money and time\",\n",
        "        \"Extremely disappointing experience\",\n",
        "        \"Horrible acting and bad screenplay\",\n",
        "    ] * 50\n",
        "\n",
        "    # Neutral reviews\n",
        "    neutral_texts = [\n",
        "        \"The movie was okay nothing special\",\n",
        "        \"Average film with some good moments\",\n",
        "        \"Decent movie but not great\",\n",
        "        \"It was fine but forgettable\",\n",
        "        \"Neither good nor bad just mediocre\",\n",
        "        \"Some parts were good others not so much\",\n",
        "        \"Could have been better\",\n",
        "        \"Standard fare nothing extraordinary\",\n",
        "        \"Passable entertainment\",\n",
        "        \"Acceptable but not memorable\",\n",
        "    ] * 50\n",
        "\n",
        "    texts = positive_texts + negative_texts + neutral_texts\n",
        "    labels = [2] * len(positive_texts) + [0] * len(negative_texts) + [1] * len(neutral_texts)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "def encode_texts_for_bert(texts, tokenizer, max_len=128):\n",
        "    \"\"\"Encode texts using the vocabulary from BERT\"\"\"\n",
        "    encoded = []\n",
        "    for text in texts:\n",
        "        tokens = text.lower().split()[:max_len - 2]\n",
        "        token_ids = [tokenizer.get(token, 2) for token in tokens]  # 2 for [UNK]\n",
        "\n",
        "        # Padding\n",
        "        if len(token_ids) < max_len:\n",
        "            token_ids = token_ids + [0] * (max_len - len(token_ids))\n",
        "\n",
        "        encoded.append(token_ids)\n",
        "\n",
        "    return np.array(encoded)\n",
        "\n",
        "# ============ Training and Evaluation ============\n",
        "\n",
        "def train_sentiment_classifier(bert_base_model, vocab, config):\n",
        "    \"\"\"Train sentiment classification model\"\"\"\n",
        "\n",
        "    # Prepare data\n",
        "    texts, labels = prepare_sentiment_data_simple()\n",
        "\n",
        "    # Split data\n",
        "    X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
        "        texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    # Encode texts\n",
        "    X_train = encode_texts_for_bert(X_train_text, vocab, config.max_len)\n",
        "    X_val = encode_texts_for_bert(X_val_text, vocab, config.max_len)\n",
        "\n",
        "    print(f\"Training samples: {len(X_train)}\")\n",
        "    print(f\"Validation samples: {len(X_val)}\")\n",
        "    print(f\"Class distribution: {np.bincount(y_train)}\")\n",
        "\n",
        "    # Create BERT encoder (without MLM head)\n",
        "    inputs = layers.Input(shape=(config.max_len,), dtype=tf.int32)\n",
        "\n",
        "    # Get embeddings from pre-trained BERT\n",
        "    for layer in bert_base_model.layers[:-1]:  # Exclude MLM head\n",
        "        if isinstance(layer, layers.InputLayer):\n",
        "            continue\n",
        "        inputs_temp = inputs if 'x' not in locals() else x\n",
        "        x = layer(inputs_temp)\n",
        "\n",
        "    bert_encoder = keras.Model(inputs=bert_base_model.input, outputs=x)\n",
        "\n",
        "    # Create classifier\n",
        "    classifier = create_sentiment_classifier(bert_encoder, config, num_classes=3)\n",
        "\n",
        "    # Compile\n",
        "    classifier.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=2e-5),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(\"\\nSentiment Classifier Architecture:\")\n",
        "    classifier.summary()\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=3, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss', factor=0.5, patience=2, min_lr=1e-7\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    history = classifier.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        batch_size=32,\n",
        "        epochs=15,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return classifier, history, X_val, y_val\n",
        "\n",
        "def evaluate_sentiment_model(model, X_val, y_val):\n",
        "    \"\"\"Evaluate the sentiment classification model\"\"\"\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_probs = model.predict(X_val)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SENTIMENT CLASSIFICATION RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    class_names = ['Negative', 'Neutral', 'Positive']\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_val, y_pred, target_names=class_names))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix - Sentiment Classification')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sentiment_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\nConfusion matrix saved as 'sentiment_confusion_matrix.png'\")\n",
        "\n",
        "    # Calculate per-class accuracy\n",
        "    print(\"\\nPer-class Accuracy:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_acc = cm[i, i] / cm[i].sum()\n",
        "        print(f\"{class_name}: {class_acc:.4f}\")\n",
        "\n",
        "    return y_pred, y_pred_probs\n",
        "\n",
        "def plot_training_history(history, task_name=\"Sentiment Classification\"):\n",
        "    \"\"\"Plot training history\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    ax1.plot(history.history['loss'], label='Training Loss')\n",
        "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    ax1.set_title(f'{task_name} - Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax2.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax2.set_title(f'{task_name} - Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{task_name.lower().replace(\" \", \"_\")}_history.png', dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\nTraining history saved as '{task_name.lower().replace(' ', '_')}_history.png'\")\n",
        "    plt.show()\n",
        "\n",
        "# ============ Comparison with Pre-trained BERT ============\n",
        "\n",
        "def compare_with_pretrained_bert():\n",
        "    \"\"\"Compare with Hugging Face BERT (if available)\"\"\"\n",
        "    try:\n",
        "        from transformers import TFBertForSequenceClassification, BertTokenizer\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"COMPARISON WITH PRE-TRAINED BERT\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Load pre-trained BERT\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = TFBertForSequenceClassification.from_pretrained(\n",
        "            'bert-base-uncased', num_labels=3\n",
        "        )\n",
        "\n",
        "        print(\"\\nPre-trained BERT-base-uncased loaded successfully\")\n",
        "        print(f\"Model parameters: ~110M\")\n",
        "\n",
        "        # You can add comparison code here\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"\\nHugging Face transformers not installed.\")\n",
        "        print(\"Install with: pip install transformers\")\n",
        "\n",
        "# ============ Main Execution ============\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Note: This assumes you have bert_model and vocab from Task 1\n",
        "    # You would load them like this:\n",
        "    # bert_model = keras.models.load_model('bert_mlm_model.h5')\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"BERT FINE-TUNING: SENTIMENT CLASSIFICATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # For demonstration, you need to run Task 1 first\n",
        "    print(\"\\nNote: Run Task 1 (MLM training) first to get bert_model and vocab\")\n",
        "    print(\"\\nExample usage:\")\n",
        "    print(\"sentiment_model, history, X_val, y_val = train_sentiment_classifier(bert_model, vocab, config)\")\n",
        "    print(\"y_pred, y_probs = evaluate_sentiment_model(sentiment_model, X_val, y_val)\")\n",
        "    print(\"plot_training_history(history)\")\n",
        "\n",
        "    # Save model\n",
        "    # sentiment_model.save('bert_sentiment_classifier.h5')\n",
        "    print(\"\\nSentiment classifier training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HacXWGyUoi84",
        "outputId": "9235afd8-f8ef-437f-f8db-fef7e6b353ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "BERT FINE-TUNING: SENTIMENT CLASSIFICATION\n",
            "==================================================\n",
            "\n",
            "Note: Run Task 1 (MLM training) first to get bert_model and vocab\n",
            "\n",
            "Example usage:\n",
            "sentiment_model, history, X_val, y_val = train_sentiment_classifier(bert_model, vocab, config)\n",
            "y_pred, y_probs = evaluate_sentiment_model(sentiment_model, X_val, y_val)\n",
            "plot_training_history(history)\n",
            "\n",
            "Sentiment classifier training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Finetuning - Question answering"
      ],
      "metadata": {
        "id": "KeexuOr-ou7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "# ============ SQuAD Data Loading ============\n",
        "\n",
        "def load_squad_dataset(file_path='squad_train.json', max_samples=1000):\n",
        "    \"\"\"\n",
        "    Load SQuAD dataset\n",
        "    Download from: https://rajpurkar.github.io/SQuAD-explorer/\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            squad_data = json.load(f)\n",
        "\n",
        "        contexts = []\n",
        "        questions = []\n",
        "        answers = []\n",
        "\n",
        "        for article in squad_data['data'][:max_samples]:\n",
        "            for paragraph in article['paragraphs']:\n",
        "                context = paragraph['context']\n",
        "                for qa in paragraph['qas']:\n",
        "                    question = qa['question']\n",
        "                    if not qa['is_impossible']:\n",
        "                        answer = qa['answers'][0]\n",
        "                        answer_text = answer['text']\n",
        "                        answer_start = answer['answer_start']\n",
        "\n",
        "                        contexts.append(context)\n",
        "                        questions.append(question)\n",
        "                        answers.append({\n",
        "                            'text': answer_text,\n",
        "                            'start': answer_start,\n",
        "                            'end': answer_start + len(answer_text)\n",
        "                        })\n",
        "\n",
        "        return contexts, questions, answers\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"SQuAD dataset not found. Using synthetic data for demonstration.\")\n",
        "        return create_synthetic_squad_data()\n",
        "\n",
        "def create_synthetic_squad_data():\n",
        "    \"\"\"Create synthetic QA data for demonstration\"\"\"\n",
        "    contexts = [\n",
        "        \"The Amazon rainforest is the largest tropical rainforest in the world. It covers much of northwestern Brazil and extends into Colombia, Peru and other South American countries. The forest is home to millions of species of plants and animals.\",\n",
        "        \"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\",\n",
        "        \"The Eiffel Tower was built in 1889 for the World's Fair in Paris. It was designed by engineer Gustave Eiffel and stands 324 meters tall. It has become a global cultural icon of France and one of the most recognizable structures in the world.\",\n",
        "        \"Photosynthesis is the process by which plants use sunlight, water and carbon dioxide to create oxygen and energy in the form of sugar. This process is crucial for life on Earth as it is the primary source of oxygen in the atmosphere.\",\n",
        "        \"The Great Wall of China was built over many centuries to protect Chinese states from invasions. The wall stretches over 13000 miles and was constructed using stone, brick, tamped earth, wood, and other materials.\",\n",
        "    ] * 100\n",
        "\n",
        "    questions = [\n",
        "        \"Where is the Amazon rainforest located?\",\n",
        "        \"What is machine learning?\",\n",
        "        \"When was the Eiffel Tower built?\",\n",
        "        \"What do plants use in photosynthesis?\",\n",
        "        \"Why was the Great Wall of China built?\",\n",
        "    ] * 100\n",
        "\n",
        "    answers = [\n",
        "        {'text': 'northwestern Brazil', 'start': 77, 'end': 96},\n",
        "        {'text': 'a method of data analysis', 'start': 18, 'end': 43},\n",
        "        {'text': '1889', 'start': 29, 'end': 33},\n",
        "        {'text': 'sunlight, water and carbon dioxide', 'start': 47, 'end': 81},\n",
        "        {'text': 'to protect Chinese states from invasions', 'start': 56, 'end': 96},\n",
        "    ] * 100\n",
        "\n",
        "    return contexts, questions, answers\n",
        "\n",
        "# ============ Data Preprocessing for QA ============\n",
        "\n",
        "def preprocess_qa_data(contexts, questions, answers, tokenizer, config):\n",
        "    \"\"\"Preprocess QA data for BERT\"\"\"\n",
        "    max_len = config.max_len\n",
        "\n",
        "    input_ids = []\n",
        "    token_type_ids = []\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for context, question, answer in zip(contexts, questions, answers):\n",
        "        # Tokenize\n",
        "        question_tokens = question.lower().split()\n",
        "        context_tokens = context.lower().split()\n",
        "\n",
        "        # Combine [CLS] question [SEP] context [SEP]\n",
        "        tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + context_tokens + ['[SEP]']\n",
        "\n",
        "        # Truncate if necessary\n",
        "        if len(tokens) > max_len:\n",
        "            tokens = tokens[:max_len]\n",
        "\n",
        "        # Convert to IDs\n",
        "        token_ids = [tokenizer.get(token, 2) for token in tokens]\n",
        "\n",
        "        # Token type IDs (0 for question, 1 for context)\n",
        "        sep_idx = tokens.index('[SEP]')\n",
        "        type_ids = [0] * (sep_idx + 1) + [1] * (len(tokens) - sep_idx - 1)\n",
        "\n",
        "        # Find answer positions in tokens\n",
        "        answer_text_tokens = answer['text'].lower().split()\n",
        "        start_pos = 0\n",
        "        end_pos = 0\n",
        "\n",
        "        # Simple position finding (improved version would use character-level)\n",
        "        for i in range(len(tokens)):\n",
        "            if tokens[i:i+len(answer_text_tokens)] == answer_text_tokens:\n",
        "                start_pos = i\n",
        "                end_pos = i + len(answer_text_tokens) - 1\n",
        "                break\n",
        "\n",
        "        # Padding\n",
        "        padding_length = max_len - len(token_ids)\n",
        "        token_ids += [0] * padding_length\n",
        "        type_ids += [0] * padding_length\n",
        "\n",
        "        input_ids.append(token_ids)\n",
        "        token_type_ids.append(type_ids)\n",
        "        start_positions.append(start_pos)\n",
        "        end_positions.append(end_pos)\n",
        "\n",
        "    return (np.array(input_ids),\n",
        "            np.array(token_type_ids),\n",
        "            np.array(start_positions),\n",
        "            np.array(end_positions))\n",
        "\n",
        "# ============ QA Model Architecture ============\n",
        "\n",
        "def create_qa_model(bert_encoder, config):\n",
        "    \"\"\"Create Question Answering model with BERT\"\"\"\n",
        "\n",
        "    # Inputs\n",
        "    input_ids = layers.Input(shape=(config.max_len,), dtype=tf.int32, name='input_ids')\n",
        "    token_type_ids = layers.Input(shape=(config.max_len,), dtype=tf.int32, name='token_type_ids')\n",
        "\n",
        "    # BERT encoding\n",
        "    bert_output = bert_encoder(input_ids)\n",
        "\n",
        "    # QA heads for start and end positions\n",
        "    start_logits = layers.Dense(1, name='start_logit')(bert_output)\n",
        "    start_logits = layers.Flatten()(start_logits)\n",
        "    start_probs = layers.Activation('softmax', name='start_position')(start_logits)\n",
        "\n",
        "    end_logits = layers.Dense(1, name='end_logit')(bert_output)\n",
        "    end_logits = layers.Flatten()(end_logits)\n",
        "    end_probs = layers.Activation('softmax', name='end_position')(end_logits)\n",
        "\n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, token_type_ids],\n",
        "        outputs=[start_probs, end_probs]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# ============ Training ============\n",
        "\n",
        "def train_qa_model(bert_encoder, vocab, config):\n",
        "    \"\"\"Train Question Answering model\"\"\"\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"LOADING SQUAD DATASET\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Load data\n",
        "    contexts, questions, answers = create_synthetic_squad_data()\n",
        "\n",
        "    print(f\"Total samples: {len(contexts)}\")\n",
        "    print(f\"Sample context: {contexts[0][:100]}...\")\n",
        "    print(f\"Sample question: {questions[0]}\")\n",
        "    print(f\"Sample answer: {answers[0]}\")\n",
        "\n",
        "    # Preprocess\n",
        "    input_ids, token_type_ids, start_pos, end_pos = preprocess_qa_data(\n",
        "        contexts, questions, answers, vocab, config\n",
        "    )\n",
        "\n",
        "    # Split data\n",
        "    split_idx = int(0.9 * len(input_ids))\n",
        "\n",
        "    train_input_ids = input_ids[:split_idx]\n",
        "    train_token_type_ids = token_type_ids[:split_idx]\n",
        "    train_start = start_pos[:split_idx]\n",
        "    train_end = end_pos[:split_idx]\n",
        "\n",
        "    val_input_ids = input_ids[split_idx:]\n",
        "    val_token_type_ids = token_type_ids[split_idx:]\n",
        "    val_start = start_pos[split_idx:]\n",
        "    val_end = end_pos[split_idx:]\n",
        "\n",
        "    print(f\"\\nTraining samples: {len(train_input_ids)}\")\n",
        "    print(f\"Validation samples: {len(val_input_ids)}\")\n",
        "\n",
        "    # Create model\n",
        "    qa_model = create_qa_model(bert_encoder, config)\n",
        "\n",
        "    # Compile\n",
        "    qa_model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=3e-5),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(\"\\nQA Model Architecture:\")\n",
        "    qa_model.summary()\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=5, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    history = qa_model.fit(\n",
        "        [train_input_ids, train_token_type_ids],\n",
        "        [train_start, train_end],\n",
        "        validation_data=(\n",
        "            [val_input_ids, val_token_type_ids],\n",
        "            [val_start, val_end]\n",
        "        ),\n",
        "        batch_size=16,\n",
        "        epochs=20,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return qa_model, history, (val_input_ids, val_token_type_ids, val_start, val_end)\n",
        "\n",
        "# ============ Evaluation ============\n",
        "\n",
        "def evaluate_qa_model(model, val_data, contexts, questions, answers):\n",
        "    \"\"\"Evaluate QA model\"\"\"\n",
        "    val_input_ids, val_token_type_ids, val_start, val_end = val_data\n",
        "\n",
        "    # Predict\n",
        "    start_probs, end_probs = model.predict([val_input_ids, val_token_type_ids])\n",
        "\n",
        "    pred_starts = np.argmax(start_probs, axis=1)\n",
        "    pred_ends = np.argmax(end_probs, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    start_accuracy = np.mean(pred_starts == val_start)\n",
        "    end_accuracy = np.mean(pred_ends == val_end)\n",
        "    exact_match = np.mean((pred_starts == val_start) & (pred_ends == val_end))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"QUESTION ANSWERING RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Start Position Accuracy: {start_accuracy:.4f}\")\n",
        "    print(f\"End Position Accuracy: {end_accuracy:.4f}\")\n",
        "    print(f\"Exact Match: {exact_match:.4f}\")\n",
        "\n",
        "    # Show some predictions\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SAMPLE PREDICTIONS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for i in range(min(5, len(val_input_ids))):\n",
        "        print(f\"\\nSample {i+1}:\")\n",
        "        print(f\"Question: {questions[-(len(val_input_ids)-i)]}\")\n",
        "        print(f\"True Answer: {answers[-(len(val_input_ids)-i)]['text']}\")\n",
        "        print(f\"Predicted Start: {pred_starts[i]}, End: {pred_ends[i]}\")\n",
        "        print(f\"True Start: {val_start[i]}, End: {val_end[i]}\")\n",
        "\n",
        "    return pred_starts, pred_ends\n",
        "\n",
        "# ============ Main Execution ============\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*50)\n",
        "    print(\"BERT FINE-TUNING: QUESTION ANSWERING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"\\nNote: Run Task 1 (MLM training) first to get bert_encoder and vocab\")\n",
        "    print(\"\\nExample usage:\")\n",
        "    print(\"qa_model, history, val_data = train_qa_model(bert_encoder, vocab, config)\")\n",
        "    print(\"pred_starts, pred_ends = evaluate_qa_model(qa_model, val_data, contexts, questions, answers)\")\n",
        "\n",
        "    # Save model\n",
        "    # qa_model.save('bert_qa_model.h5')\n",
        "    print(\"\\nQuestion Answering model training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3iq6Tg_oySh",
        "outputId": "6b38f7bc-7bc0-4393-e60e-91803f15d57a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "BERT FINE-TUNING: QUESTION ANSWERING\n",
            "==================================================\n",
            "\n",
            "Note: Run Task 1 (MLM training) first to get bert_encoder and vocab\n",
            "\n",
            "Example usage:\n",
            "qa_model, history, val_data = train_qa_model(bert_encoder, vocab, config)\n",
            "pred_starts, pred_ends = evaluate_qa_model(qa_model, val_data, contexts, questions, answers)\n",
            "\n",
            "Question Answering model training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Finetuning - Semantic Similarity"
      ],
      "metadata": {
        "id": "R-JxBnXgo8K3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ============ SNLI Data Loading ============\n",
        "\n",
        "def load_snli_dataset(file_path='snli_1.0_train.txt', max_samples=5000):\n",
        "    \"\"\"\n",
        "    Load SNLI (Stanford Natural Language Inference) dataset\n",
        "    Download from: https://nlp.stanford.edu/projects/snli/\n",
        "\n",
        "    Labels: 0 = entailment, 1 = neutral, 2 = contradiction\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, sep='\\t')\n",
        "\n",
        "        # Filter out '-' labels\n",
        "        df = df[df['gold_label'] != '-']\n",
        "\n",
        "        # Map labels\n",
        "        label_map = {\n",
        "            'entailment': 0,\n",
        "            'neutral': 1,\n",
        "            'contradiction': 2\n",
        "        }\n",
        "\n",
        "        df['label'] = df['gold_label'].map(label_map)\n",
        "\n",
        "        # Select columns\n",
        "        sentence1 = df['sentence1'].tolist()[:max_samples]\n",
        "        sentence2 = df['sentence2'].tolist()[:max_samples]\n",
        "        labels = df['label'].tolist()[:max_samples]\n",
        "\n",
        "        return sentence1, sentence2, labels\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"SNLI dataset not found. Using synthetic data for demonstration.\")\n",
        "        return create_synthetic_snli_data()\n",
        "\n",
        "def create_synthetic_snli_data():\n",
        "    \"\"\"Create synthetic semantic similarity data\"\"\"\n",
        "\n",
        "    # Entailment pairs (sentence1 -> sentence2)\n",
        "    entailment_pairs = [\n",
        "        (\"A man is playing guitar\", \"A person is making music\"),\n",
        "        (\"The dog is running in the park\", \"An animal is moving outdoors\"),\n",
        "        (\"She is reading a book\", \"A woman is looking at text\"),\n",
        "        (\"The car is moving fast\", \"A vehicle is in motion\"),\n",
        "        (\"Children are playing soccer\", \"Kids are engaged in sports\"),\n",
        "        (\"A woman is cooking dinner\", \"Someone is preparing food\"),\n",
        "        (\"The cat is sleeping on the couch\", \"A pet is resting on furniture\"),\n",
        "        (\"He is writing an email\", \"A person is typing a message\"),\n",
        "        (\"Birds are flying in the sky\", \"Animals are moving through the air\"),\n",
        "        (\"Students are studying in the library\", \"People are learning in a quiet place\"),\n",
        "    ] * 50\n",
        "\n",
        "    # Neutral pairs (related but not entailment)\n",
        "    neutral_pairs = [\n",
        "        (\"A man is playing guitar\", \"The man is very talented\"),\n",
        "        (\"The dog is running in the park\", \"The weather is nice today\"),\n",
        "        (\"She is reading a book\", \"Books are interesting\"),\n",
        "        (\"The car is moving fast\", \"Traffic is heavy\"),\n",
        "        (\"Children are playing soccer\", \"Soccer is a popular sport\"),\n",
        "        (\"A woman is cooking dinner\", \"The kitchen is clean\"),\n",
        "        (\"The cat is sleeping on the couch\", \"Cats are independent animals\"),\n",
        "        (\"He is writing an email\", \"Email is convenient\"),\n",
        "        (\"Birds are flying in the sky\", \"The sky is blue\"),\n",
        "        (\"Students are studying in the library\", \"The library has many books\"),\n",
        "    ] * 50\n",
        "\n",
        "    # Contradiction pairs\n",
        "    contradiction_pairs = [\n",
        "        (\"A man is playing guitar\", \"The man is swimming\"),\n",
        "        (\"The dog is running in the park\", \"The dog is sleeping indoors\"),\n",
        "        (\"She is reading a book\", \"She is watching television\"),\n",
        "        (\"The car is moving fast\", \"The car is parked\"),\n",
        "        (\"Children are playing soccer\", \"Children are sitting quietly\"),\n",
        "        (\"A woman is cooking dinner\", \"The woman is eating at a restaurant\"),\n",
        "        (\"The cat is sleeping on the couch\", \"The cat is climbing a tree\"),\n",
        "        (\"He is writing an email\", \"He is talking on the phone\"),\n",
        "        (\"Birds are flying in the sky\", \"Birds are swimming underwater\"),\n",
        "        (\"Students are studying in the library\", \"Students are playing outside\"),\n",
        "    ] * 50\n",
        "\n",
        "    # Combine all pairs\n",
        "    sentence1 = []\n",
        "    sentence2 = []\n",
        "    labels = []\n",
        "\n",
        "    for s1, s2 in entailment_pairs:\n",
        "        sentence1.append(s1)\n",
        "        sentence2.append(s2)\n",
        "        labels.append(0)  # entailment\n",
        "\n",
        "    for s1, s2 in neutral_pairs:\n",
        "        sentence1.append(s1)\n",
        "        sentence2.append(s2)\n",
        "        labels.append(1)  # neutral\n",
        "\n",
        "    for s1, s2 in contradiction_pairs:\n",
        "        sentence1.append(s1)\n",
        "        sentence2.append(s2)\n",
        "        labels.append(2)  # contradiction\n",
        "\n",
        "    return sentence1, sentence2, labels\n",
        "\n",
        "# ============ Data Preprocessing ============\n",
        "\n",
        "def encode_sentence_pair(sentence1, sentence2, tokenizer, max_len=128):\n",
        "    \"\"\"Encode sentence pairs for BERT\"\"\"\n",
        "    # Tokenize\n",
        "    tokens1 = sentence1.lower().split()\n",
        "    tokens2 = sentence2.lower().split()\n",
        "\n",
        "    # Combine [CLS] sent1 [SEP] sent2 [SEP]\n",
        "    combined = ['[CLS]'] + tokens1 + ['[SEP]'] + tokens2 + ['[SEP]']\n",
        "\n",
        "    # Truncate if necessary\n",
        "    if len(combined) > max_len:\n",
        "        # Keep [CLS] and both [SEP], truncate sentences equally\n",
        "        available = max_len - 3  # for [CLS] and 2x[SEP]\n",
        "        tokens1 = tokens1[:available//2]\n",
        "        tokens2 = tokens2[:available//2]\n",
        "        combined = ['[CLS]'] + tokens1 + ['[SEP]'] + tokens2 + ['[SEP]']\n",
        "\n",
        "    # Convert to IDs\n",
        "    token_ids = [tokenizer.get(token, 2) for token in combined]\n",
        "\n",
        "    # Padding\n",
        "    if len(token_ids) < max_len:\n",
        "        token_ids = token_ids + [0] * (max_len - len(token_ids))\n",
        "\n",
        "    return token_ids\n",
        "\n",
        "def preprocess_snli_data(sentence1_list, sentence2_list, labels, tokenizer, max_len):\n",
        "    \"\"\"Preprocess all SNLI data\"\"\"\n",
        "    encoded_pairs = []\n",
        "\n",
        "    for s1, s2 in zip(sentence1_list, sentence2_list):\n",
        "        encoded = encode_sentence_pair(s1, s2, tokenizer, max_len)\n",
        "        encoded_pairs.append(encoded)\n",
        "\n",
        "    return np.array(encoded_pairs), np.array(labels)\n",
        "\n",
        "# ============ Semantic Similarity Model ============\n",
        "\n",
        "def create_semantic_similarity_model(bert_encoder, config, num_classes=3):\n",
        "    \"\"\"\n",
        "    Create semantic similarity model\n",
        "    num_classes: 3 (entailment, neutral, contradiction)\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = layers.Input(shape=(config.max_len,), dtype=tf.int32)\n",
        "\n",
        "    # BERT encoding\n",
        "    bert_output = bert_encoder(inputs)\n",
        "\n",
        "    # Use [CLS] token representation\n",
        "    cls_token = bert_output[:, 0, :]\n",
        "\n",
        "    # Classification layers\n",
        "    x = layers.Dropout(0.3)(cls_token)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax', name='similarity_output')(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# ============ Training ============\n",
        "\n",
        "def train_semantic_similarity_model(bert_encoder, vocab, config):\n",
        "    \"\"\"Train semantic similarity model on SNLI\"\"\"\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"LOADING SNLI DATASET\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Load data\n",
        "    sentence1, sentence2, labels = create_synthetic_snli_data()\n",
        "\n",
        "    print(f\"Total samples: {len(sentence1)}\")\n",
        "    print(f\"Label distribution: {np.bincount(labels)}\")\n",
        "    print(\"\\nSample pairs:\")\n",
        "    for i in range(3):\n",
        "        label_name = ['Entailment', 'Neutral', 'Contradiction'][labels[i]]\n",
        "        print(f\"\\n{label_name}:\")\n",
        "        print(f\"  Premise: {sentence1[i]}\")\n",
        "        print(f\"  Hypothesis: {sentence2[i]}\")\n",
        "\n",
        "    # Preprocess\n",
        "    X, y = preprocess_snli_data(sentence1, sentence2, labels, vocab, config.max_len)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.15, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining samples: {len(X_train)}\")\n",
        "    print(f\"Validation samples: {len(X_val)}\")\n",
        "\n",
        "    # Create model\n",
        "    similarity_model = create_semantic_similarity_model(bert_encoder, config)\n",
        "\n",
        "    # Compile\n",
        "    similarity_model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=2e-5),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(\"\\nSemantic Similarity Model Architecture:\")\n",
        "    similarity_model.summary()\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=5, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    history = similarity_model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        batch_size=32,\n",
        "        epochs=20,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return similarity_model, history, X_val, y_val, sentence1, sentence2\n",
        "\n",
        "# ============ Evaluation ============\n",
        "\n",
        "def evaluate_semantic_similarity(model, X_val, y_val):\n",
        "    \"\"\"Evaluate semantic similarity model\"\"\"\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_probs = model.predict(X_val)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SEMANTIC SIMILARITY RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Overall accuracy\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Classification report\n",
        "    class_names = ['Entailment', 'Neutral', 'Contradiction']\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_val, y_pred, target_names=class_names, digits=4))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd',\n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.title('Confusion Matrix - Semantic Similarity (SNLI)', fontsize=14, pad=20)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('semantic_similarity_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\nConfusion matrix saved as 'semantic_similarity_confusion_matrix.png'\")\n",
        "\n",
        "    # Per-class metrics\n",
        "    print(\"\\nPer-class Performance:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        precision = cm[i, i] / (cm[:, i].sum() + 1e-10)\n",
        "        recall = cm[i, i] / (cm[i, :].sum() + 1e-10)\n",
        "        f1 = 2 * precision * recall / (precision + recall + 1e-10)\n",
        "        print(f\"{class_name:15s} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    return y_pred, y_pred_probs\n",
        "\n",
        "def plot_training_curves(history):\n",
        "    \"\"\"Plot training history\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Loss\n",
        "    ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "    ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "    ax1.set_title('Semantic Similarity - Training Loss', fontsize=14)\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontsize=12)\n",
        "    ax1.legend(fontsize=10)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy\n",
        "    ax2.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "    ax2.set_title('Semantic Similarity - Training Accuracy', fontsize=14)\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Accuracy', fontsize=12)\n",
        "    ax2.legend(fontsize=10)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('semantic_similarity_training_history.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\nTraining history saved as 'semantic_similarity_training_history.png'\")\n",
        "    plt.show()\n",
        "\n",
        "# ============ Inference Function ============\n",
        "\n",
        "def predict_similarity(model, sentence1, sentence2, tokenizer, config):\n",
        "    \"\"\"Predict semantic similarity between two sentences\"\"\"\n",
        "    encoded = encode_sentence_pair(sentence1, sentence2, tokenizer, config.max_len)\n",
        "    encoded = np.array([encoded])\n",
        "\n",
        "    probs = model.predict(encoded, verbose=0)[0]\n",
        "    prediction = np.argmax(probs)\n",
        "\n",
        "    labels = ['Entailment', 'Neutral', 'Contradiction']\n",
        "\n",
        "    print(f\"\\nSentence 1: {sentence1}\")\n",
        "    print(f\"Sentence 2: {sentence2}\")\n",
        "    print(f\"\\nPrediction: {labels[prediction]}\")\n",
        "    print(f\"Confidence: {probs[prediction]:.4f}\")\n",
        "    print(\"\\nProbabilities:\")\n",
        "    for label, prob in zip(labels, probs):\n",
        "        print(f\"  {label:15s}: {prob:.4f}\")\n",
        "\n",
        "    return prediction, probs\n",
        "\n",
        "# ============ Main Execution ============\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*50)\n",
        "    print(\"BERT FINE-TUNING: SEMANTIC SIMILARITY\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"\\nNote: Run Task 1 (MLM training) first to get bert_encoder and vocab\")\n",
        "    print(\"\\nExample usage:\")\n",
        "    print(\"sim_model, history, X_val, y_val, s1, s2 = train_semantic_similarity_model(bert_encoder, vocab, config)\")\n",
        "    print(\"y_pred, y_probs = evaluate_semantic_similarity(sim_model, X_val, y_val)\")\n",
        "    print(\"plot_training_curves(history)\")\n",
        "\n",
        "    # Example inference\n",
        "    print(\"\\n\\nExample inference:\")\n",
        "    print(\"predict_similarity(sim_model, 'A dog is running', 'An animal is moving', vocab, config)\")\n",
        "\n",
        "    # Save model\n",
        "    # sim_model.save('bert_semantic_similarity.h5')\n",
        "    print(\"\\nSemantic similarity model training complete!\")"
      ],
      "metadata": {
        "id": "Y7Wr3UMgpBdo",
        "outputId": "d18efca9-a4b6-42c6-c54c-1c088533309f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "BERT FINE-TUNING: SEMANTIC SIMILARITY\n",
            "==================================================\n",
            "\n",
            "Note: Run Task 1 (MLM training) first to get bert_encoder and vocab\n",
            "\n",
            "Example usage:\n",
            "sim_model, history, X_val, y_val, s1, s2 = train_semantic_similarity_model(bert_encoder, vocab, config)\n",
            "y_pred, y_probs = evaluate_semantic_similarity(sim_model, X_val, y_val)\n",
            "plot_training_curves(history)\n",
            "\n",
            "\n",
            "Example inference:\n",
            "predict_similarity(sim_model, 'A dog is running', 'An animal is moving', vocab, config)\n",
            "\n",
            "Semantic similarity model training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Comparison"
      ],
      "metadata": {
        "id": "7b725XpurrZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "BULLETPROOF BERT COMPARISON\n",
        "Works even if HuggingFace/TF Hub fail - uses PyTorch backend\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# ============================================================================\n",
        "# OPTION 1: Use PyTorch HuggingFace (Most Reliable)\n",
        "# ============================================================================\n",
        "\n",
        "def load_huggingface_bert_pytorch(num_classes=3):\n",
        "    \"\"\"\n",
        "    Use PyTorch version of BERT (more stable than TensorFlow)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from transformers import BertForSequenceClassification, BertTokenizer\n",
        "        import torch\n",
        "\n",
        "        print(\"Loading HuggingFace BERT (PyTorch backend)...\")\n",
        "\n",
        "        # Use smaller model that fits on any GPU\n",
        "        model_name = 'google/bert_uncased_L-4_H-512_A-8'  # 29M params\n",
        "        # Alternative: 'google/bert_uncased_L-2_H-128_A-2'  # 4M params\n",
        "\n",
        "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        model = BertForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_classes\n",
        "        )\n",
        "\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "        print(f\"✅ Loaded: {model_name}\")\n",
        "        print(f\"   Parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
        "        print(f\"   Device: {device}\")\n",
        "\n",
        "        return model, tokenizer, total_params, device\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"❌ PyTorch transformers not available\")\n",
        "        print(\"Install: pip install torch transformers\")\n",
        "        return None, None, 29_000_000, None\n",
        "\n",
        "def evaluate_pytorch_bert(model, tokenizer, X_test, y_test, device):\n",
        "    \"\"\"Evaluate PyTorch BERT model\"\"\"\n",
        "    import torch\n",
        "\n",
        "    print(\"\\nEvaluating HuggingFace BERT...\")\n",
        "\n",
        "    # Convert numpy to torch\n",
        "    X_tensor = torch.tensor(X_test, dtype=torch.long).to(device)\n",
        "\n",
        "    predictions = []\n",
        "    batch_size = 32\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(X_tensor), batch_size):\n",
        "            batch = X_tensor[i:i+batch_size]\n",
        "            outputs = model(batch)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "    inference_time = time.time() - start_time\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions, average='macro')\n",
        "\n",
        "    print(f\"✅ HuggingFace BERT Results:\")\n",
        "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"   F1-Score: {f1:.4f}\")\n",
        "    print(f\"   Inference Time: {inference_time:.3f}s\")\n",
        "    print(f\"   Speed: {len(X_test)/inference_time:.1f} samples/sec\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_score': f1,\n",
        "        'inference_time': inference_time,\n",
        "        'samples_per_sec': len(X_test) / inference_time\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# OPTION 2: Manual Comparison (If All Libraries Fail)\n",
        "# ============================================================================\n",
        "\n",
        "def manual_comparison_with_literature(custom_results, task_name='sentiment'):\n",
        "    \"\"\"\n",
        "    Compare with published BERT results from papers\n",
        "    Use this if you can't load HuggingFace models\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMPARISON WITH PUBLISHED BERT RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Published BERT results on common benchmarks\n",
        "    # Source: BERT paper (Devlin et al., 2018) and subsequent papers\n",
        "\n",
        "    literature_results = {\n",
        "        'sentiment': {\n",
        "            'BERT-tiny (2L-128d)': {'accuracy': 0.79, 'params_M': 4.4},\n",
        "            'BERT-small (4L-512d)': {'accuracy': 0.87, 'params_M': 29},\n",
        "            'BERT-base (12L-768d)': {'accuracy': 0.93, 'params_M': 110},\n",
        "        },\n",
        "        'qa': {\n",
        "            'BERT-tiny (2L-128d)': {'f1': 0.68, 'params_M': 4.4},\n",
        "            'BERT-small (4L-512d)': {'f1': 0.78, 'params_M': 29},\n",
        "            'BERT-base (12L-768d)': {'f1': 0.88, 'params_M': 110},\n",
        "        },\n",
        "        'similarity': {\n",
        "            'BERT-tiny (2L-128d)': {'accuracy': 0.75, 'params_M': 4.4},\n",
        "            'BERT-small (4L-512d)': {'accuracy': 0.84, 'params_M': 29},\n",
        "            'BERT-base (12L-768d)': {'accuracy': 0.90, 'params_M': 110},\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if task_name not in literature_results:\n",
        "        task_name = 'sentiment'\n",
        "\n",
        "    lit_data = literature_results[task_name]\n",
        "\n",
        "    print(f\"\\nTask: {task_name.upper()}\")\n",
        "    print(\"\\nPublished BERT Results:\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for model_name, results in lit_data.items():\n",
        "        print(f\"{model_name:30s}: \", end=\"\")\n",
        "        for metric, value in results.items():\n",
        "            if 'params' in metric:\n",
        "                print(f\"{metric}: {value}M  \", end=\"\")\n",
        "            else:\n",
        "                print(f\"{metric}: {value:.4f}  \", end=\"\")\n",
        "        print()\n",
        "\n",
        "    return lit_data\n",
        "\n",
        "# ============================================================================\n",
        "# OPTION 3: Simple TensorFlow Comparison (Your Custom Model)\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_custom_bert_simple(model, X_test, y_test):\n",
        "    \"\"\"Evaluate your custom BERT model\"\"\"\n",
        "\n",
        "    print(\"\\nEvaluating Custom BERT...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    predictions = model.predict(X_test, verbose=0)\n",
        "    inference_time = time.time() - start_time\n",
        "\n",
        "    pred_labels = np.argmax(predictions, axis=1)\n",
        "    accuracy = accuracy_score(y_test, pred_labels)\n",
        "    f1 = f1_score(y_test, pred_labels, average='macro')\n",
        "\n",
        "    print(f\"✅ Custom BERT Results:\")\n",
        "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"   F1-Score: {f1:.4f}\")\n",
        "    print(f\"   Inference Time: {inference_time:.3f}s\")\n",
        "    print(f\"   Speed: {len(X_test)/inference_time:.1f} samples/sec\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_score': f1,\n",
        "        'inference_time': inference_time,\n",
        "        'samples_per_sec': len(X_test) / inference_time\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN COMPARISON FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def compare_bert_models(custom_model, X_test, y_test,\n",
        "                       custom_params, custom_config, task_name='sentiment'):\n",
        "    \"\"\"\n",
        "    Complete comparison pipeline\n",
        "    Tries PyTorch HF BERT, falls back to literature comparison\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"BERT PERFORMANCE COMPARISON - {task_name.upper()}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Evaluate custom model\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 1: EVALUATING CUSTOM BERT\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    custom_results = evaluate_custom_bert_simple(custom_model, X_test, y_test)\n",
        "    custom_results['parameters_M'] = custom_params / 1e6\n",
        "    custom_results['config'] = custom_config\n",
        "    custom_results['train_time_min'] = 35  # Replace with your actual training time\n",
        "\n",
        "    # Try to evaluate HuggingFace BERT\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 2: LOADING PUBLIC BERT\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    hf_model, tokenizer, hf_params, device = load_huggingface_bert_pytorch(\n",
        "        num_classes=len(np.unique(y_test))\n",
        "    )\n",
        "\n",
        "    hf_results = None\n",
        "\n",
        "    if hf_model is not None:\n",
        "        try:\n",
        "            hf_results = evaluate_pytorch_bert(hf_model, tokenizer, X_test, y_test, device)\n",
        "            hf_results['parameters_M'] = hf_params / 1e6\n",
        "            hf_results['train_time_min'] = 150  # Typical for BERT-small\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Evaluation failed: {e}\")\n",
        "            hf_results = None\n",
        "\n",
        "    # Display comparison\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 3: COMPARISON RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if hf_results:\n",
        "        # Real comparison with loaded model\n",
        "        comparison_df = pd.DataFrame([\n",
        "            {\n",
        "                'Model': f'Custom BERT ({custom_config})',\n",
        "                'Parameters (M)': custom_results['parameters_M'],\n",
        "                'Accuracy': custom_results['accuracy'],\n",
        "                'F1-Score': custom_results['f1_score'],\n",
        "                'Inference (s)': custom_results['inference_time'],\n",
        "                'Speed (samples/s)': custom_results['samples_per_sec']\n",
        "            },\n",
        "            {\n",
        "                'Model': 'HuggingFace BERT',\n",
        "                'Parameters (M)': hf_results['parameters_M'],\n",
        "                'Accuracy': hf_results['accuracy'],\n",
        "                'F1-Score': hf_results['f1_score'],\n",
        "                'Inference (s)': hf_results['inference_time'],\n",
        "                'Speed (samples/s)': hf_results['samples_per_sec']\n",
        "            }\n",
        "        ])\n",
        "\n",
        "        print(\"\\n\" + comparison_df.to_string(index=False))\n",
        "\n",
        "        # Analysis\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"ANALYSIS\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        param_ratio = hf_results['parameters_M'] / custom_results['parameters_M']\n",
        "        acc_diff = (hf_results['accuracy'] - custom_results['accuracy']) * 100\n",
        "        speed_ratio = custom_results['samples_per_sec'] / hf_results['samples_per_sec']\n",
        "\n",
        "        print(f\"\\n📊 Size Comparison:\")\n",
        "        print(f\"   HF BERT is {param_ratio:.1f}x larger\")\n",
        "        print(f\"   Custom uses {100*(1-1/param_ratio):.1f}% fewer parameters\")\n",
        "\n",
        "        print(f\"\\n📊 Performance:\")\n",
        "        print(f\"   Accuracy difference: {acc_diff:+.2f}%\")\n",
        "        print(f\"   Custom achieves {(custom_results['accuracy']/hf_results['accuracy'])*100:.1f}% of HF BERT accuracy\")\n",
        "\n",
        "        print(f\"\\n📊 Speed:\")\n",
        "        print(f\"   Custom is {speed_ratio:.1f}x faster\")\n",
        "\n",
        "    else:\n",
        "        # Comparison with literature\n",
        "        print(\"\\n⚠️  Could not load HuggingFace model\")\n",
        "        print(\"Comparing with published results instead...\")\n",
        "\n",
        "        lit_results = manual_comparison_with_literature(custom_results, task_name)\n",
        "\n",
        "        # Compare with BERT-small from literature\n",
        "        bert_small = lit_results.get('BERT-small (4L-512d)', {})\n",
        "\n",
        "        if bert_small:\n",
        "            print(f\"\\n\" + \"=\"*70)\n",
        "            print(\"COMPARISON WITH BERT-SMALL (Published Results)\")\n",
        "            print(\"=\"*70)\n",
        "\n",
        "            comparison_df = pd.DataFrame([\n",
        "                {\n",
        "                    'Model': f'Custom BERT ({custom_config})',\n",
        "                    'Parameters (M)': custom_results['parameters_M'],\n",
        "                    'Accuracy': custom_results['accuracy'],\n",
        "                    'F1-Score': custom_results['f1_score'],\n",
        "                },\n",
        "                {\n",
        "                    'Model': 'BERT-small (Literature)',\n",
        "                    'Parameters (M)': bert_small['params_M'],\n",
        "                    'Accuracy': bert_small.get('accuracy', bert_small.get('f1', 0)),\n",
        "                    'F1-Score': bert_small.get('f1', bert_small.get('accuracy', 0)),\n",
        "                }\n",
        "            ])\n",
        "\n",
        "            print(\"\\n\" + comparison_df.to_string(index=False))\n",
        "\n",
        "            # Analysis\n",
        "            lit_metric = bert_small.get('accuracy', bert_small.get('f1', 0))\n",
        "            custom_metric = custom_results['accuracy']\n",
        "\n",
        "            print(f\"\\n📊 Your custom BERT achieves {(custom_metric/lit_metric)*100:.1f}% of published BERT-small performance\")\n",
        "            print(f\"📊 Difference: {(lit_metric - custom_metric)*100:+.2f}%\")\n",
        "\n",
        "    # Create visualization\n",
        "    plot_comparison_simple(custom_results, hf_results, lit_results if not hf_results else None, task_name)\n",
        "\n",
        "    # Save results\n",
        "    if hf_results:\n",
        "        comparison_df.to_csv(f'bert_comparison_{task_name}.csv', index=False)\n",
        "\n",
        "    return custom_results, hf_results\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def plot_comparison_simple(custom_results, hf_results, lit_results, task_name):\n",
        "    \"\"\"Create comparison plot\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    fig.suptitle(f'BERT Comparison - {task_name.upper()}', fontsize=16, fontweight='bold')\n",
        "\n",
        "    if hf_results:\n",
        "        # Plot 1: Accuracy\n",
        "        ax1 = axes[0]\n",
        "        models = ['Custom', 'HuggingFace']\n",
        "        accs = [custom_results['accuracy'], hf_results['accuracy']]\n",
        "        colors = ['steelblue', 'coral']\n",
        "        bars = ax1.bar(models, accs, color=colors, alpha=0.7)\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.set_title('Accuracy Comparison')\n",
        "        ax1.set_ylim([0, 1])\n",
        "        for bar, acc in zip(bars, accs):\n",
        "            height = bar.get_height()\n",
        "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{acc:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # Plot 2: Parameters\n",
        "        ax2 = axes[1]\n",
        "        params = [custom_results['parameters_M'], hf_results['parameters_M']]\n",
        "        bars = ax2.bar(models, params, color=colors, alpha=0.7)\n",
        "        ax2.set_ylabel('Parameters (Millions)')\n",
        "        ax2.set_title('Model Size')\n",
        "        for bar, p in zip(bars, params):\n",
        "            height = bar.get_height()\n",
        "            ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{p:.1f}M', ha='center', va='bottom')\n",
        "\n",
        "        # Plot 3: Speed\n",
        "        ax3 = axes[2]\n",
        "        speeds = [custom_results['samples_per_sec'], hf_results['samples_per_sec']]\n",
        "        bars = ax3.bar(models, speeds, color=colors, alpha=0.7)\n",
        "        ax3.set_ylabel('Samples per Second')\n",
        "        ax3.set_title('Inference Speed')\n",
        "        for bar, s in zip(bars, speeds):\n",
        "            height = bar.get_height()\n",
        "            ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{s:.1f}', ha='center', va='bottom')\n",
        "\n",
        "    else:\n",
        "        # Compare with literature\n",
        "        ax1 = axes[0]\n",
        "        models = ['Custom'] + list(lit_results.keys())\n",
        "        accs = [custom_results['accuracy']] + [r.get('accuracy', r.get('f1', 0)) for r in lit_results.values()]\n",
        "        ax1.bar(models, accs, alpha=0.7)\n",
        "        ax1.set_ylabel('Accuracy/F1')\n",
        "        ax1.set_title('Performance vs Literature')\n",
        "        ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        ax2 = axes[1]\n",
        "        params = [custom_results['parameters_M']] + [r['params_M'] for r in lit_results.values()]\n",
        "        ax2.bar(models, params, alpha=0.7)\n",
        "        ax2.set_ylabel('Parameters (M)')\n",
        "        ax2.set_title('Model Size')\n",
        "        ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        ax3 = axes[2]\n",
        "        ax3.text(0.5, 0.5, 'Literature comparison\\n(no speed data)',\n",
        "                ha='center', va='center', transform=ax3.transAxes)\n",
        "        ax3.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'bert_comparison_{task_name}.png', dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\n📊 Plot saved: bert_comparison_{task_name}.png\")\n",
        "    plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# COMPLETE EXAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "def main_example():\n",
        "    \"\"\"Complete example with dummy data\"\"\"\n",
        "\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"BERT COMPARISON - COMPLETE EXAMPLE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Generate dummy data\n",
        "    print(\"\\nGenerating sample data...\")\n",
        "    X_test = np.random.randint(0, 5000, size=(300, 128))\n",
        "    y_test = np.random.randint(0, 3, size=300)\n",
        "\n",
        "    # Create dummy custom model\n",
        "    print(\"Creating dummy custom BERT...\")\n",
        "    inputs = keras.Input(shape=(128,), dtype=tf.int32)\n",
        "    x = keras.layers.Embedding(5000, 256)(inputs)\n",
        "    x = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(x)\n",
        "    x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "    x = keras.layers.Dense(128, activation='relu')(x)\n",
        "    outputs = keras.layers.Dense(3, activation='softmax')(x)\n",
        "    custom_model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    custom_params = sum([tf.size(w).numpy() for w in custom_model.trainable_weights])\n",
        "    custom_config = \"256d-2L\"\n",
        "\n",
        "    # Run comparison\n",
        "    custom_results, hf_results = compare_bert_models(\n",
        "        custom_model, X_test, y_test,\n",
        "        custom_params, custom_config, 'sentiment'\n",
        "    )\n",
        "\n",
        "    print(\"\\n✅ Comparison complete!\")\n",
        "\n",
        "    return custom_results, hf_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main_example()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TO USE WITH YOUR ACTUAL MODEL:\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\"\"\n",
        "# 1. Load your trained model\n",
        "custom_model = keras.models.load_model('your_bert_model.h5')\n",
        "\n",
        "# 2. Load your test data\n",
        "X_test = ...  # your test data\n",
        "y_test = ...  # your test labels\n",
        "\n",
        "# 3. Get model parameters\n",
        "custom_params = sum([tf.size(w).numpy() for w in custom_model.trainable_weights])\n",
        "\n",
        "# 4. Run comparison\n",
        "results = compare_bert_models(\n",
        "    custom_model, X_test, y_test,\n",
        "    custom_params, \"256d-4L\", \"sentiment\"\n",
        ")\n",
        "    \"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1945f942725d462c99ceec2048a06d6e",
            "cc61a9e7cc1e4cad887f8444d3429b19",
            "9989cbc2f12e45f381bd574997f2e26a",
            "a00ff78f646943b58e666a30f3cb2cf5",
            "7c8812d10d3c412c8b9a3e32580b7b24",
            "cee84e88245d4d32a25bb582555aaf82",
            "23008180619a4131a7985aef5aa2853a",
            "89020ed5c2124c92b8a0495625bd5f54",
            "0bce41423ad74dd6a3456467f7ef583e",
            "dc358f7657994113930c26887ad0a080",
            "b5a8494ea02e4597b6d3e7f3e101de65"
          ]
        },
        "id": "uQbNpIhhr43j",
        "outputId": "681ab642-19a2-4b71-9614-8839970dcebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "BERT COMPARISON - COMPLETE EXAMPLE\n",
            "======================================================================\n",
            "\n",
            "Generating sample data...\n",
            "Creating dummy custom BERT...\n",
            "\n",
            "======================================================================\n",
            "BERT PERFORMANCE COMPARISON - SENTIMENT\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "STEP 1: EVALUATING CUSTOM BERT\n",
            "======================================================================\n",
            "\n",
            "Evaluating Custom BERT...\n",
            "✅ Custom BERT Results:\n",
            "   Accuracy: 0.3133\n",
            "   F1-Score: 0.3022\n",
            "   Inference Time: 5.280s\n",
            "   Speed: 56.8 samples/sec\n",
            "\n",
            "======================================================================\n",
            "STEP 2: LOADING PUBLIC BERT\n",
            "======================================================================\n",
            "Loading HuggingFace BERT (PyTorch backend)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-512_A-8 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded: google/bert_uncased_L-4_H-512_A-8\n",
            "   Parameters: 28,765,187 (28.8M)\n",
            "   Device: cuda\n",
            "\n",
            "Evaluating HuggingFace BERT...\n",
            "✅ HuggingFace BERT Results:\n",
            "   Accuracy: 0.3533\n",
            "   F1-Score: 0.1811\n",
            "   Inference Time: 0.653s\n",
            "   Speed: 459.2 samples/sec\n",
            "\n",
            "======================================================================\n",
            "STEP 3: COMPARISON RESULTS\n",
            "======================================================================\n",
            "\n",
            "                Model  Parameters (M)  Accuracy  F1-Score  Inference (s)  Speed (samples/s)\n",
            "Custom BERT (256d-2L)        1.707523  0.313333  0.302164       5.279840          56.819899\n",
            "     HuggingFace BERT       28.765187  0.353333  0.181147       0.653279         459.221817\n",
            "\n",
            "======================================================================\n",
            "ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "📊 Size Comparison:\n",
            "   HF BERT is 16.8x larger\n",
            "   Custom uses 94.1% fewer parameters\n",
            "\n",
            "📊 Performance:\n",
            "   Accuracy difference: +4.00%\n",
            "   Custom achieves 88.7% of HF BERT accuracy\n",
            "\n",
            "📊 Speed:\n",
            "   Custom is 0.1x faster\n",
            "\n",
            "📊 Plot saved: bert_comparison_sentiment.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Comparison complete!\n",
            "\n",
            "======================================================================\n",
            "TO USE WITH YOUR ACTUAL MODEL:\n",
            "======================================================================\n",
            "\n",
            "# 1. Load your trained model\n",
            "custom_model = keras.models.load_model('your_bert_model.h5')\n",
            "\n",
            "# 2. Load your test data\n",
            "X_test = ...  # your test data\n",
            "y_test = ...  # your test labels\n",
            "\n",
            "# 3. Get model parameters\n",
            "custom_params = sum([tf.size(w).numpy() for w in custom_model.trainable_weights])\n",
            "\n",
            "# 4. Run comparison\n",
            "results = compare_bert_models(\n",
            "    custom_model, X_test, y_test,\n",
            "    custom_params, \"256d-4L\", \"sentiment\"\n",
            ")\n",
            "    \n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset of 20 classes from cifar 100"
      ],
      "metadata": {
        "id": "RfRkegs5MyDP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWseskI0MhGh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.datasets import cifar100\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def display_img(img_set, title_set):\n",
        "    n = len(title_set)\n",
        "    for i in range(n):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(img_set[i])\n",
        "        plt.title(title_set[i])\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FVyiHQRrNFQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(trainX, trainY), (testX, testY) = cifar100.load_data(label_mode='coarse')\n",
        "\n",
        "print('trainX.shape: {}, trainY.shape: {}, testX.shape: {}, testY.shape: {}'.format(trainX.shape, trainY.shape, testX.shape, testY.shape))\n",
        "print('trainX.dtype: {}, trainY.dtype: {}, testX.dtype: {}, testY.dtype: {}'.format(trainX.dtype, trainY.dtype, testX.dtype, testY.dtype))\n",
        "print('trainX.Range: {} - {}, testX.Range: {} - {}'.format(trainX.max(), trainX.min(), testX.max(), testX.min()))\n",
        "\n",
        "coarse_label_names = [\n",
        "    'aquatic mammals', 'fish', 'flowers', 'food containers', 'fruit and vegetables',\n",
        "    'household electrical devices', 'household furniture', 'insects', 'large carnivores',\n",
        "    'large man-made outdoor things', 'large natural outdoor scenes', 'large omnivores and herbivores',\n",
        "    'medium-sized mammals', 'non-insect invertebrates', 'people', 'reptiles',\n",
        "    'small mammals', 'trees', 'vehicles 1', 'vehicles 2'\n",
        "]\n",
        "\n",
        "display_img(trainX[:9], [coarse_label_names[label[0]] for label in trainY[:9]])"
      ],
      "metadata": {
        "id": "fS6hAljHNKUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Process the data"
      ],
      "metadata": {
        "id": "X2rAMdqhNQqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the pixel values to [0, 1]\n",
        "trainX = trainX.astype('float32') / 255.0\n",
        "testX = testX.astype('float32') / 255.0\n",
        "\n",
        "# Resize images to 96x96 (required by most ImageNet pre-trained models)\n",
        "trainX = tf.image.resize(trainX, (96, 96))\n",
        "testX = tf.image.resize(testX, (96, 96))\n",
        "\n",
        "# Convert labels to one-hot encoding (20 classes)\n",
        "trainY = to_categorical(trainY, num_classes=20)\n",
        "testY = to_categorical(testY, num_classes=20)\n",
        "\n",
        "# Print data shapes and types\n",
        "print(f\"trainX.shape: {trainX.shape}, testX.shape: {testX.shape}\")\n",
        "print(f\"trainX.dtype: {trainX.dtype}, testX.dtype: {testX.dtype}\")\n",
        "print(f\"trainY.shape: {trainY.shape}, testY.shape: {testY.shape}\")\n",
        "print(f\"trainY.dtype: {trainY.dtype}, testY.dtype: {testY.dtype}\")\n",
        "\n",
        "# Check sample labels\n",
        "print(\"Sample trainY (one-hot encoded):\")\n",
        "print(trainY[:5])"
      ],
      "metadata": {
        "id": "ve90Wky3NNZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. DenseNet121"
      ],
      "metadata": {
        "id": "HRR7ecWgNY53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "\n",
        "# Load DenseNet121 base model\n",
        "base_model = DenseNet121(\n",
        "    input_shape=(96, 96, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# Freeze base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification head\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(20, activation='softmax')(x)\n",
        "\n",
        "model_densenet = Model(inputs=base_model.input, outputs=predictions)\n",
        "model_densenet.summary()\n"
      ],
      "metadata": {
        "id": "hl5NR5srNbYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_densenet.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_transfer = model_densenet.fit(\n",
        "    trainX, trainY,\n",
        "    validation_split=0.1,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "transfer_loss, transfer_acc = model_densenet.evaluate(testX, testY, verbose=0)\n",
        "print(f\"ðŸ“Š DenseNet121 Transfer Learning Accuracy: {transfer_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "f9LwIZj2Nexe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# Unfreeze the last 40 layers of the base model\n",
        "for layer in model_densenet.layers[-40:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile with a low learning rate\n",
        "model_densenet.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "]\n",
        "\n",
        "history_finetune = model_densenet.fit(\n",
        "    trainX, trainY,\n",
        "    validation_split=0.1,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "finetune_loss, finetune_acc = model_densenet.evaluate(testX, testY, verbose=0)\n",
        "print(f\"ðŸ“Š DenseNet121 Fine-Tuning Accuracy: {finetune_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "cnzR2f8rNiea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. DenseNet201"
      ],
      "metadata": {
        "id": "usrdRn1bRqbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import DenseNet201\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "\n",
        "# Load DenseNet201 base model\n",
        "base_model = DenseNet201(\n",
        "    input_shape=(96, 96, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# Freeze all layers initially\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification head\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(20, activation='softmax')(x)  # 20-class output\n",
        "\n",
        "model_densenet201 = Model(inputs=base_model.input, outputs=predictions)\n",
        "model_densenet201.summary()\n"
      ],
      "metadata": {
        "id": "tw5lf-QnRo-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_densenet201.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_transfer = model_densenet201.fit(\n",
        "    trainX, trainY,\n",
        "    validation_split=0.1,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "transfer_loss, transfer_acc = model_densenet201.evaluate(testX, testY, verbose=0)\n",
        "print(f\"ðŸ“Š DenseNet201 Transfer Learning Accuracy: {transfer_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "iddbnedJR7NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# Unfreeze last 40 layers for fine-tuning\n",
        "for layer in model_densenet201.layers[-40:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile with lower learning rate\n",
        "model_densenet201.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "]\n",
        "\n",
        "history_finetune = model_densenet201.fit(\n",
        "    trainX, trainY,\n",
        "    validation_split=0.1,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "finetune_loss, finetune_acc = model_densenet201.evaluate(testX, testY, verbose=0)\n",
        "print(f\"ðŸ“Š DenseNet201 Fine-Tuning Accuracy: {finetune_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "re7SuCwaSBf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. DenseNet169"
      ],
      "metadata": {
        "id": "vupxMHw9jUuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import DenseNet169\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "\n",
        "# Load DenseNet201 base model\n",
        "base_model = DenseNet169(\n",
        "    input_shape=(96, 96, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# Freeze all layers initially\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification head\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(20, activation='softmax')(x)  # 20-class output\n",
        "\n",
        "model_densenet169 = Model(inputs=base_model.input, outputs=predictions)\n",
        "model_densenet169.summary()\n"
      ],
      "metadata": {
        "id": "3fDa7W2pi418"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_densenet169.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_transfer = model_densenet169.fit(\n",
        "    trainX, trainY,\n",
        "    validation_split=0.1,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "transfer_loss, transfer_acc = model_densenet169.evaluate(testX, testY, verbose=0)\n",
        "print(f\"ðŸ“Š DenseNet201 Transfer Learning Accuracy: {transfer_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZFXqAFYbjerk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# Unfreeze last 40 layers for fine-tuning\n",
        "for layer in model_densenet169.layers[-40:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile with lower learning rate\n",
        "model_densenet169.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "]\n",
        "\n",
        "history_finetune = model_densenet169.fit(\n",
        "    trainX, trainY,\n",
        "    validation_split=0.1,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "finetune_loss, finetune_acc = model_densenet169.evaluate(testX, testY, verbose=0)\n",
        "print(f\"ðŸ“Š DenseNet201 Fine-Tuning Accuracy: {finetune_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "PIfu3sTAjowf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. VGG19"
      ],
      "metadata": {
        "id": "gtrrD9xyl_71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "\n",
        "vgg_base = VGG19(input_shape=(96, 96, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# Freeze all layers in VGG19\n",
        "for layer in vgg_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification head\n",
        "x = vgg_base.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Dense(20, activation='softmax', name='OutputLayer')(x)\n",
        "\n",
        "# Final model\n",
        "model_vgg19 = Model(inputs=vgg_base.input, outputs=outputs, name='VGG19_CIFAR100_20Classes')\n",
        "model_vgg19.summary()"
      ],
      "metadata": {
        "id": "t7Brc_E6mMn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model for transfer learning\n",
        "model_vgg19.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train only the new head\n",
        "history_vgg_transfer = model_vgg19.fit(\n",
        "    trainX, trainY,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "vgg_transfer_loss, vgg_transfer_acc = model_vgg19.evaluate(testX, testY, verbose=0)\n",
        "print(f\"ðŸ“Š VGG19 Transfer Learning Test Accuracy: {vgg_transfer_acc:.4f}\")"
      ],
      "metadata": {
        "id": "z0_2Sn8GmeIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze last few layers of VGG19 (e.g., last 4 conv blocks)\n",
        "for layer in model_vgg19.layers[-40:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile with lower learning rate\n",
        "model_vgg19.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "history_vgg_finetune = model_vgg19.fit(\n",
        "    trainX, trainY,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate fine-tuned model\n",
        "vgg_finetune_loss, vgg_finetune_acc = model_vgg19.evaluate(testX, testY, verbose=0)\n",
        "print(f\"ðŸ“Š VGG19 Fine-Tuning Test Accuracy: {vgg_finetune_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "B1N6gA2YuZIN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
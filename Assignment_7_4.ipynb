{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "595536a3",
      "metadata": {
        "id": "595536a3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6384ee7a",
      "metadata": {
        "id": "6384ee7a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define the YOLOv1 network\n",
        "class YOLOv1(nn.Module):\n",
        "    def __init__(self, S=7, B=2, C=1):\n",
        "        super(YOLOv1, self).__init__()\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(192, 128, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024 * S * S, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, S * S * (B * 5 + C))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x).view(-1, self.S, self.S, self.B * 5 + self.C)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d2d8c5d",
      "metadata": {
        "id": "8d2d8c5d",
        "outputId": "1f3c83f5-9614-498a-c548-dcbff921a581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,472\n",
            "              ReLU-2         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-3           [-1, 64, 56, 56]               0\n",
            "            Conv2d-4          [-1, 192, 56, 56]         110,784\n",
            "              ReLU-5          [-1, 192, 56, 56]               0\n",
            "         MaxPool2d-6          [-1, 192, 28, 28]               0\n",
            "            Conv2d-7          [-1, 128, 28, 28]          24,704\n",
            "              ReLU-8          [-1, 128, 28, 28]               0\n",
            "            Conv2d-9          [-1, 256, 28, 28]         295,168\n",
            "             ReLU-10          [-1, 256, 28, 28]               0\n",
            "        MaxPool2d-11          [-1, 256, 14, 14]               0\n",
            "           Conv2d-12          [-1, 256, 14, 14]         590,080\n",
            "             ReLU-13          [-1, 256, 14, 14]               0\n",
            "           Conv2d-14          [-1, 512, 14, 14]       1,180,160\n",
            "             ReLU-15          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-16            [-1, 512, 7, 7]               0\n",
            "           Conv2d-17            [-1, 512, 7, 7]       2,359,808\n",
            "             ReLU-18            [-1, 512, 7, 7]               0\n",
            "           Conv2d-19           [-1, 1024, 7, 7]       4,719,616\n",
            "             ReLU-20           [-1, 1024, 7, 7]               0\n",
            "           Conv2d-21           [-1, 1024, 7, 7]       9,438,208\n",
            "             ReLU-22           [-1, 1024, 7, 7]               0\n",
            "           Conv2d-23           [-1, 1024, 7, 7]       9,438,208\n",
            "             ReLU-24           [-1, 1024, 7, 7]               0\n",
            "          Flatten-25                [-1, 50176]               0\n",
            "           Linear-26                 [-1, 4096]     205,524,992\n",
            "             ReLU-27                 [-1, 4096]               0\n",
            "          Dropout-28                 [-1, 4096]               0\n",
            "           Linear-29                  [-1, 539]       2,208,283\n",
            "================================================================\n",
            "Total params: 235,899,483\n",
            "Trainable params: 235,899,483\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 34.74\n",
            "Params size (MB): 899.89\n",
            "Estimated Total Size (MB): 935.20\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "model = YOLOv1()\n",
        "model.to('cuda')  # move model to GPU\n",
        "summary(model, (3, 224, 224), device='cuda')  # specify CUDA device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e620c5c0",
      "metadata": {
        "id": "e620c5c0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class WiderFaceDataset(Dataset):\n",
        "    def __init__(self, img_dir, label_dir, S=7, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform\n",
        "        self.S = S\n",
        "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.img_files[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        label_path = os.path.join(self.label_dir, os.path.splitext(img_name)[0] + \".txt\")\n",
        "\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        h, w = img.size\n",
        "        target = torch.zeros((self.S, self.S, 5 * 2 + 1))  # B=2, C=1\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f:\n",
        "                    cls, x_center, y_center, width, height = map(float, line.strip().split())\n",
        "                    grid_x = min(int(x_center * self.S), self.S - 1)\n",
        "                    grid_y = min(int(y_center * self.S), self.S - 1)\n",
        "                    # Assign to first predictor (simplified)\n",
        "                    target[grid_y, grid_x, 0] = x_center * self.S - grid_x\n",
        "                    target[grid_y, grid_x, 1] = y_center * self.S - grid_y\n",
        "                    target[grid_y, grid_x, 2] = width * self.S\n",
        "                    target[grid_y, grid_x, 3] = height * self.S\n",
        "                    target[grid_y, grid_x, 4] = 1.0  # Confidence\n",
        "                    target[grid_y, grid_x, 10] = cls\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b23918f",
      "metadata": {
        "id": "6b23918f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def yolo_loss(preds, targets, S=7, B=2, lambda_coord=5.0, lambda_noobj=0.5):\n",
        "    batch_size = preds.size(0)\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        pred = preds[i].view(S, S, B * 5 + 1)  # [S, S, B*5 + C]\n",
        "        target = targets[i].view(S, S, B * 5 + 1)\n",
        "\n",
        "        for j in range(S):\n",
        "            for k in range(S):\n",
        "                # Objectness score (confidence)\n",
        "                obj_mask = target[j, k, 4] > 0  # True if object exists\n",
        "                noobj_mask = ~obj_mask\n",
        "\n",
        "                if obj_mask:\n",
        "                    # Coordinate loss (x, y, w, h) for the best bounding box\n",
        "                    best_iou = 0\n",
        "                    best_box_idx = 0\n",
        "                    for b in range(B):\n",
        "                        box_pred = pred[j, k, 5 * b:5 * (b + 1)]\n",
        "                        x, y, w, h, conf = box_pred\n",
        "                        target_box = target[j, k, :5]\n",
        "                        iou = calculate_iou((x, y, w, h), (target_box[0], target_box[1], target_box[2], target_box[3]))\n",
        "                        if iou > best_iou:\n",
        "                            best_iou = iou\n",
        "                            best_box_idx = b\n",
        "\n",
        "                    best_pred = pred[j, k, 5 * best_box_idx:5 * (best_box_idx + 1)]\n",
        "                    target_box = target[j, k, :5]\n",
        "                    coord_loss = nn.MSELoss()(best_pred[:4], target_box[:4]) * lambda_coord\n",
        "                    conf_loss = nn.MSELoss()(best_pred[4], target_box[4])\n",
        "                    class_loss = nn.MSELoss()(pred[j, k, 10], target[j, k, 10])\n",
        "\n",
        "                    total_loss += coord_loss + conf_loss + class_loss\n",
        "                # No object loss\n",
        "                for b in range(B):\n",
        "                    noobj_conf = pred[j, k, 5 * b + 4]\n",
        "                    zero_tensor = torch.tensor(0.0, device=pred.device)\n",
        "                    total_loss += nn.MSELoss()(noobj_conf, zero_tensor) * lambda_noobj * noobj_mask.float()\n",
        "\n",
        "    return total_loss / batch_size\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    # box: (x_center, y_center, width, height)\n",
        "    x1, y1, w1, h1 = box1\n",
        "    x2, y2, w2, h2 = box2\n",
        "    w1_half = w1 / 2\n",
        "    h1_half = h1 / 2\n",
        "    w2_half = w2 / 2\n",
        "    h2_half = h2 / 2\n",
        "\n",
        "    x1_min = x1 - w1_half\n",
        "    y1_min = y1 - h1_half\n",
        "    x1_max = x1 + w1_half\n",
        "    y1_max = y1 + h1_half\n",
        "\n",
        "    x2_min = x2 - w2_half\n",
        "    y2_min = y2 - h2_half\n",
        "    x2_max = x2 + w2_half\n",
        "    y2_max = y2 + h2_half\n",
        "\n",
        "    inter_x_min = max(x1_min, x2_min)\n",
        "    inter_y_min = max(y1_min, y2_min)\n",
        "    inter_x_max = min(x1_max, x2_max)\n",
        "    inter_y_max = min(y1_max, y2_max)\n",
        "\n",
        "    inter_area = max(0, inter_x_max - inter_x_min) * max(0, inter_y_max - inter_y_min)\n",
        "    union_area = w1 * h1 + w2 * h2 - inter_area\n",
        "\n",
        "    return inter_area / union_area if union_area > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b97c062",
      "metadata": {
        "id": "6b97c062"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Hyperparameters\n",
        "S = 7\n",
        "B = 2\n",
        "C = 1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "image_size = 224\n",
        "\n",
        "# Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Dataset and DataLoader\n",
        "train_img_dir =   \"/home/cse/Desktop/Apurbo/datasets/WiderFace/archive/WIDER Face Dataset For YOLOv12/WIDER Face Dataset For YOLOv12/train/images\"  # Adjust path\n",
        "train_label_dir = \"/home/cse/Desktop/Apurbo/datasets/WiderFace/archive/WIDER Face Dataset For YOLOv12/WIDER Face Dataset For YOLOv12/train/labels\"  # Adjust path\n",
        "val_img_dir =     \"/home/cse/Desktop/Apurbo/datasets/WiderFace/archive/WIDER Face Dataset For YOLOv12/WIDER Face Dataset For YOLOv12/val/images\"  # Adjust path\n",
        "val_label_dir =   \"/home/cse/Desktop/Apurbo/datasets/WiderFace/archive/WIDER Face Dataset For YOLOv12/WIDER Face Dataset For YOLOv12/val/labels\"  # Adjust paths\n",
        "\n",
        "train_dataset = WiderFaceDataset(train_img_dir, train_label_dir, S=S, transform=transform)\n",
        "val_dataset = WiderFaceDataset(val_img_dir, val_label_dir, S=S, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = YOLOv1(S=S, B=B, C=C).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Metric\n",
        "metric = MeanAveragePrecision().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97197e31",
      "metadata": {
        "id": "97197e31",
        "outputId": "f293e603-572a-4625-b152-01e6ff79f2dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [0/805], Loss: 12.6961\n",
            "Epoch [1/10], Step [10/805], Loss: 13.9542\n",
            "Epoch [1/10], Step [20/805], Loss: 11.0309\n",
            "Epoch [1/10], Step [30/805], Loss: 7.0369\n",
            "Epoch [1/10], Step [40/805], Loss: 7.2621\n",
            "Epoch [1/10], Step [50/805], Loss: 11.4739\n",
            "Epoch [1/10], Step [60/805], Loss: 7.0607\n",
            "Epoch [1/10], Step [70/805], Loss: 11.4232\n",
            "Epoch [1/10], Step [80/805], Loss: 8.2501\n",
            "Epoch [1/10], Step [90/805], Loss: 10.6414\n",
            "Epoch [1/10], Step [100/805], Loss: 8.0437\n",
            "Epoch [1/10], Step [110/805], Loss: 13.1536\n",
            "Epoch [1/10], Step [120/805], Loss: 8.5123\n",
            "Epoch [1/10], Step [130/805], Loss: 8.4753\n",
            "Epoch [1/10], Step [140/805], Loss: 13.8317\n",
            "Epoch [1/10], Step [150/805], Loss: 8.5037\n",
            "Epoch [1/10], Step [160/805], Loss: 9.4029\n",
            "Epoch [1/10], Step [170/805], Loss: 6.3362\n",
            "Epoch [1/10], Step [180/805], Loss: 7.3290\n",
            "Epoch [1/10], Step [190/805], Loss: 11.3190\n",
            "Epoch [1/10], Step [200/805], Loss: 7.8236\n",
            "Epoch [1/10], Step [210/805], Loss: 13.6501\n",
            "Epoch [1/10], Step [220/805], Loss: 7.8885\n",
            "Epoch [1/10], Step [230/805], Loss: 8.7223\n",
            "Epoch [1/10], Step [240/805], Loss: 12.9672\n",
            "Epoch [1/10], Step [250/805], Loss: 7.6738\n",
            "Epoch [1/10], Step [260/805], Loss: 7.8764\n",
            "Epoch [1/10], Step [270/805], Loss: 5.5469\n",
            "Epoch [1/10], Step [280/805], Loss: 7.9920\n",
            "Epoch [1/10], Step [290/805], Loss: 5.2615\n",
            "Epoch [1/10], Step [300/805], Loss: 8.3842\n",
            "Epoch [1/10], Step [310/805], Loss: 7.2999\n",
            "Epoch [1/10], Step [320/805], Loss: 9.2837\n",
            "Epoch [1/10], Step [330/805], Loss: 8.7315\n",
            "Epoch [1/10], Step [340/805], Loss: 7.5568\n",
            "Epoch [1/10], Step [350/805], Loss: 7.9783\n",
            "Epoch [1/10], Step [360/805], Loss: 7.4052\n",
            "Epoch [1/10], Step [370/805], Loss: 9.5876\n",
            "Epoch [1/10], Step [380/805], Loss: 5.7036\n",
            "Epoch [1/10], Step [390/805], Loss: 9.1448\n",
            "Epoch [1/10], Step [400/805], Loss: 7.8713\n",
            "Epoch [1/10], Step [410/805], Loss: 10.3184\n",
            "Epoch [1/10], Step [420/805], Loss: 5.8079\n",
            "Epoch [1/10], Step [430/805], Loss: 8.8024\n",
            "Epoch [1/10], Step [440/805], Loss: 8.2603\n",
            "Epoch [1/10], Step [450/805], Loss: 11.2576\n",
            "Epoch [1/10], Step [460/805], Loss: 7.6951\n",
            "Epoch [1/10], Step [470/805], Loss: 7.6016\n",
            "Epoch [1/10], Step [480/805], Loss: 9.6722\n",
            "Epoch [1/10], Step [490/805], Loss: 12.0740\n",
            "Epoch [1/10], Step [500/805], Loss: 7.6431\n",
            "Epoch [1/10], Step [510/805], Loss: 13.3361\n",
            "Epoch [1/10], Step [520/805], Loss: 5.8429\n",
            "Epoch [1/10], Step [530/805], Loss: 8.4648\n",
            "Epoch [1/10], Step [540/805], Loss: 6.8604\n",
            "Epoch [1/10], Step [550/805], Loss: 10.0965\n",
            "Epoch [1/10], Step [560/805], Loss: 7.9423\n",
            "Epoch [1/10], Step [570/805], Loss: 10.6994\n",
            "Epoch [1/10], Step [580/805], Loss: 7.5070\n",
            "Epoch [1/10], Step [590/805], Loss: 6.6717\n",
            "Epoch [1/10], Step [600/805], Loss: 7.3873\n",
            "Epoch [1/10], Step [610/805], Loss: 8.4055\n",
            "Epoch [1/10], Step [620/805], Loss: 5.7048\n",
            "Epoch [1/10], Step [630/805], Loss: 10.8408\n",
            "Epoch [1/10], Step [640/805], Loss: 9.4854\n",
            "Epoch [1/10], Step [650/805], Loss: 6.7470\n",
            "Epoch [1/10], Step [660/805], Loss: 10.9369\n",
            "Epoch [1/10], Step [670/805], Loss: 10.0981\n",
            "Epoch [1/10], Step [680/805], Loss: 7.7789\n",
            "Epoch [1/10], Step [690/805], Loss: 10.0048\n",
            "Epoch [1/10], Step [700/805], Loss: 7.5374\n",
            "Epoch [1/10], Step [710/805], Loss: 11.5381\n",
            "Epoch [1/10], Step [720/805], Loss: 7.5136\n",
            "Epoch [1/10], Step [730/805], Loss: 8.6957\n",
            "Epoch [1/10], Step [740/805], Loss: 6.0842\n",
            "Epoch [1/10], Step [750/805], Loss: 5.2385\n",
            "Epoch [1/10], Step [760/805], Loss: 7.1345\n",
            "Epoch [1/10], Step [770/805], Loss: 7.0582\n",
            "Epoch [1/10], Step [780/805], Loss: 9.9414\n",
            "Epoch [1/10], Step [790/805], Loss: 9.6322\n",
            "Epoch [1/10], Step [800/805], Loss: 10.6767\n",
            "Epoch [1/10], Average Loss: 12.9607\n",
            "Epoch [2/10], Step [0/805], Loss: 6.6987\n",
            "Epoch [2/10], Step [10/805], Loss: 8.8555\n",
            "Epoch [2/10], Step [20/805], Loss: 10.6395\n",
            "Epoch [2/10], Step [30/805], Loss: 10.0561\n",
            "Epoch [2/10], Step [40/805], Loss: 7.0277\n",
            "Epoch [2/10], Step [50/805], Loss: 9.1612\n",
            "Epoch [2/10], Step [60/805], Loss: 8.5580\n",
            "Epoch [2/10], Step [70/805], Loss: 11.2330\n",
            "Epoch [2/10], Step [80/805], Loss: 5.8020\n",
            "Epoch [2/10], Step [90/805], Loss: 8.2018\n",
            "Epoch [2/10], Step [100/805], Loss: 7.5591\n",
            "Epoch [2/10], Step [110/805], Loss: 7.1928\n",
            "Epoch [2/10], Step [120/805], Loss: 6.2427\n",
            "Epoch [2/10], Step [130/805], Loss: 8.0775\n",
            "Epoch [2/10], Step [140/805], Loss: 6.1605\n",
            "Epoch [2/10], Step [150/805], Loss: 8.8780\n",
            "Epoch [2/10], Step [160/805], Loss: 7.5316\n",
            "Epoch [2/10], Step [170/805], Loss: 10.0506\n",
            "Epoch [2/10], Step [180/805], Loss: 443.9086\n",
            "Epoch [2/10], Step [190/805], Loss: 9.9792\n",
            "Epoch [2/10], Step [200/805], Loss: 9.6963\n",
            "Epoch [2/10], Step [210/805], Loss: 10.6075\n",
            "Epoch [2/10], Step [220/805], Loss: 9.2013\n",
            "Epoch [2/10], Step [230/805], Loss: 7.9237\n",
            "Epoch [2/10], Step [240/805], Loss: 12.5522\n",
            "Epoch [2/10], Step [250/805], Loss: 11.7619\n",
            "Epoch [2/10], Step [260/805], Loss: 8.8193\n",
            "Epoch [2/10], Step [270/805], Loss: 8.2797\n",
            "Epoch [2/10], Step [280/805], Loss: 7.6578\n",
            "Epoch [2/10], Step [290/805], Loss: 9.5245\n",
            "Epoch [2/10], Step [300/805], Loss: 17.9890\n",
            "Epoch [2/10], Step [310/805], Loss: 7.0257\n",
            "Epoch [2/10], Step [320/805], Loss: 5.1676\n",
            "Epoch [2/10], Step [330/805], Loss: 10.7853\n",
            "Epoch [2/10], Step [340/805], Loss: 11.8351\n",
            "Epoch [2/10], Step [350/805], Loss: 11.7025\n",
            "Epoch [2/10], Step [360/805], Loss: 8.3795\n",
            "Epoch [2/10], Step [370/805], Loss: 6.7246\n",
            "Epoch [2/10], Step [380/805], Loss: 10.8484\n",
            "Epoch [2/10], Step [390/805], Loss: 5.9357\n",
            "Epoch [2/10], Step [400/805], Loss: 8.5786\n",
            "Epoch [2/10], Step [410/805], Loss: 7.5979\n",
            "Epoch [2/10], Step [420/805], Loss: 8.3995\n",
            "Epoch [2/10], Step [430/805], Loss: 10.2435\n",
            "Epoch [2/10], Step [440/805], Loss: 7.5408\n",
            "Epoch [2/10], Step [450/805], Loss: 6.9799\n",
            "Epoch [2/10], Step [460/805], Loss: 12.3119\n",
            "Epoch [2/10], Step [470/805], Loss: 7.3292\n",
            "Epoch [2/10], Step [480/805], Loss: 8.9126\n",
            "Epoch [2/10], Step [490/805], Loss: 8.8053\n",
            "Epoch [2/10], Step [500/805], Loss: 7.0101\n",
            "Epoch [2/10], Step [510/805], Loss: 12.0538\n",
            "Epoch [2/10], Step [520/805], Loss: 11.3402\n",
            "Epoch [2/10], Step [530/805], Loss: 7.8929\n",
            "Epoch [2/10], Step [540/805], Loss: 7.0119\n",
            "Epoch [2/10], Step [550/805], Loss: 12.6901\n",
            "Epoch [2/10], Step [560/805], Loss: 8.4075\n",
            "Epoch [2/10], Step [570/805], Loss: 12.3323\n",
            "Epoch [2/10], Step [580/805], Loss: 6.1389\n",
            "Epoch [2/10], Step [590/805], Loss: 6.0691\n",
            "Epoch [2/10], Step [600/805], Loss: 7.8771\n",
            "Epoch [2/10], Step [610/805], Loss: 5.3575\n",
            "Epoch [2/10], Step [620/805], Loss: 7.8279\n",
            "Epoch [2/10], Step [630/805], Loss: 6.3646\n",
            "Epoch [2/10], Step [640/805], Loss: 7.6394\n",
            "Epoch [2/10], Step [650/805], Loss: 9.6175\n",
            "Epoch [2/10], Step [660/805], Loss: 6.8180\n",
            "Epoch [2/10], Step [670/805], Loss: 9.3626\n",
            "Epoch [2/10], Step [680/805], Loss: 8.3492\n",
            "Epoch [2/10], Step [690/805], Loss: 9.8977\n",
            "Epoch [2/10], Step [700/805], Loss: 14.2785\n",
            "Epoch [2/10], Step [710/805], Loss: 7.5722\n",
            "Epoch [2/10], Step [720/805], Loss: 10.9574\n",
            "Epoch [2/10], Step [730/805], Loss: 6.1543\n",
            "Epoch [2/10], Step [740/805], Loss: 7.6796\n",
            "Epoch [2/10], Step [750/805], Loss: 4.7078\n",
            "Epoch [2/10], Step [760/805], Loss: 6.4984\n",
            "Epoch [2/10], Step [770/805], Loss: 11.4855\n",
            "Epoch [2/10], Step [780/805], Loss: 9.3928\n",
            "Epoch [2/10], Step [790/805], Loss: 11.3889\n",
            "Epoch [2/10], Step [800/805], Loss: 9.4494\n",
            "Epoch [2/10], Average Loss: 10.3154\n",
            "Epoch [3/10], Step [0/805], Loss: 8.6882\n",
            "Epoch [3/10], Step [10/805], Loss: 6.8382\n",
            "Epoch [3/10], Step [20/805], Loss: 6.6582\n",
            "Epoch [3/10], Step [30/805], Loss: 7.9771\n",
            "Epoch [3/10], Step [40/805], Loss: 7.0027\n",
            "Epoch [3/10], Step [50/805], Loss: 11.5091\n",
            "Epoch [3/10], Step [60/805], Loss: 7.0938\n",
            "Epoch [3/10], Step [70/805], Loss: 11.2592\n",
            "Epoch [3/10], Step [80/805], Loss: 9.6945\n",
            "Epoch [3/10], Step [90/805], Loss: 6.0255\n",
            "Epoch [3/10], Step [100/805], Loss: 6.8079\n",
            "Epoch [3/10], Step [110/805], Loss: 10.1556\n",
            "Epoch [3/10], Step [120/805], Loss: 7.7389\n",
            "Epoch [3/10], Step [130/805], Loss: 9.0629\n",
            "Epoch [3/10], Step [140/805], Loss: 5.2151\n",
            "Epoch [3/10], Step [150/805], Loss: 5.1533\n",
            "Epoch [3/10], Step [160/805], Loss: 10.5178\n",
            "Epoch [3/10], Step [170/805], Loss: 10.6609\n",
            "Epoch [3/10], Step [180/805], Loss: 10.0982\n",
            "Epoch [3/10], Step [190/805], Loss: 6.9299\n",
            "Epoch [3/10], Step [200/805], Loss: 9.2754\n",
            "Epoch [3/10], Step [210/805], Loss: 6.8337\n",
            "Epoch [3/10], Step [220/805], Loss: 8.3612\n",
            "Epoch [3/10], Step [230/805], Loss: 8.1390\n",
            "Epoch [3/10], Step [240/805], Loss: 8.7248\n",
            "Epoch [3/10], Step [250/805], Loss: 7.9367\n",
            "Epoch [3/10], Step [260/805], Loss: 6.7532\n",
            "Epoch [3/10], Step [270/805], Loss: 6.3797\n",
            "Epoch [3/10], Step [280/805], Loss: 6.6513\n",
            "Epoch [3/10], Step [290/805], Loss: 7.9506\n",
            "Epoch [3/10], Step [300/805], Loss: 10.5029\n",
            "Epoch [3/10], Step [310/805], Loss: 6.2704\n",
            "Epoch [3/10], Step [320/805], Loss: 10.9626\n",
            "Epoch [3/10], Step [330/805], Loss: 11.8083\n",
            "Epoch [3/10], Step [340/805], Loss: 11.3442\n",
            "Epoch [3/10], Step [350/805], Loss: 5.8870\n",
            "Epoch [3/10], Step [360/805], Loss: 7.9419\n",
            "Epoch [3/10], Step [370/805], Loss: 12.0057\n",
            "Epoch [3/10], Step [380/805], Loss: 8.3691\n",
            "Epoch [3/10], Step [390/805], Loss: 10.1447\n",
            "Epoch [3/10], Step [400/805], Loss: 6.9244\n",
            "Epoch [3/10], Step [410/805], Loss: 8.3030\n",
            "Epoch [3/10], Step [420/805], Loss: 5.9120\n",
            "Epoch [3/10], Step [430/805], Loss: 6.6597\n",
            "Epoch [3/10], Step [440/805], Loss: 5.6905\n",
            "Epoch [3/10], Step [450/805], Loss: 8.2556\n",
            "Epoch [3/10], Step [460/805], Loss: 8.2413\n",
            "Epoch [3/10], Step [470/805], Loss: 8.5153\n",
            "Epoch [3/10], Step [480/805], Loss: 9.3069\n",
            "Epoch [3/10], Step [490/805], Loss: 8.0999\n",
            "Epoch [3/10], Step [500/805], Loss: 5.4584\n",
            "Epoch [3/10], Step [510/805], Loss: 8.4728\n",
            "Epoch [3/10], Step [520/805], Loss: 7.2720\n",
            "Epoch [3/10], Step [530/805], Loss: 8.3252\n",
            "Epoch [3/10], Step [540/805], Loss: 11.8604\n",
            "Epoch [3/10], Step [550/805], Loss: 8.6988\n",
            "Epoch [3/10], Step [560/805], Loss: 7.4245\n",
            "Epoch [3/10], Step [570/805], Loss: 12.7435\n",
            "Epoch [3/10], Step [580/805], Loss: 4.8834\n",
            "Epoch [3/10], Step [590/805], Loss: 8.7503\n",
            "Epoch [3/10], Step [600/805], Loss: 5.5312\n",
            "Epoch [3/10], Step [610/805], Loss: 6.5143\n",
            "Epoch [3/10], Step [620/805], Loss: 5.6939\n",
            "Epoch [3/10], Step [630/805], Loss: 8.1430\n",
            "Epoch [3/10], Step [640/805], Loss: 7.8305\n",
            "Epoch [3/10], Step [650/805], Loss: 6.6588\n",
            "Epoch [3/10], Step [660/805], Loss: 9.4047\n",
            "Epoch [3/10], Step [670/805], Loss: 5.5280\n",
            "Epoch [3/10], Step [680/805], Loss: 12.7279\n",
            "Epoch [3/10], Step [690/805], Loss: 6.4146\n",
            "Epoch [3/10], Step [700/805], Loss: 9.3164\n",
            "Epoch [3/10], Step [710/805], Loss: 7.8806\n",
            "Epoch [3/10], Step [720/805], Loss: 11.4270\n",
            "Epoch [3/10], Step [730/805], Loss: 12.5567\n",
            "Epoch [3/10], Step [740/805], Loss: 8.6254\n",
            "Epoch [3/10], Step [750/805], Loss: 10.0195\n",
            "Epoch [3/10], Step [760/805], Loss: 6.8145\n",
            "Epoch [3/10], Step [770/805], Loss: 8.8854\n",
            "Epoch [3/10], Step [780/805], Loss: 9.2353\n",
            "Epoch [3/10], Step [790/805], Loss: 8.8882\n",
            "Epoch [3/10], Step [800/805], Loss: 6.5976\n",
            "Epoch [3/10], Average Loss: 8.5427\n",
            "Epoch [4/10], Step [0/805], Loss: 5.3268\n",
            "Epoch [4/10], Step [10/805], Loss: 9.9248\n",
            "Epoch [4/10], Step [20/805], Loss: 10.0326\n",
            "Epoch [4/10], Step [30/805], Loss: 10.7013\n",
            "Epoch [4/10], Step [40/805], Loss: 8.1763\n",
            "Epoch [4/10], Step [50/805], Loss: 6.9436\n",
            "Epoch [4/10], Step [60/805], Loss: 7.0968\n",
            "Epoch [4/10], Step [70/805], Loss: 10.5673\n",
            "Epoch [4/10], Step [80/805], Loss: 5.3287\n",
            "Epoch [4/10], Step [90/805], Loss: 5.8502\n",
            "Epoch [4/10], Step [100/805], Loss: 8.2324\n",
            "Epoch [4/10], Step [110/805], Loss: 8.5741\n",
            "Epoch [4/10], Step [120/805], Loss: 7.9931\n",
            "Epoch [4/10], Step [130/805], Loss: 9.4430\n",
            "Epoch [4/10], Step [140/805], Loss: 9.6703\n",
            "Epoch [4/10], Step [150/805], Loss: 10.7636\n",
            "Epoch [4/10], Step [160/805], Loss: 7.5018\n",
            "Epoch [4/10], Step [170/805], Loss: 13.5369\n",
            "Epoch [4/10], Step [180/805], Loss: 7.4051\n",
            "Epoch [4/10], Step [190/805], Loss: 6.7209\n",
            "Epoch [4/10], Step [200/805], Loss: 12.5928\n",
            "Epoch [4/10], Step [210/805], Loss: 11.3004\n",
            "Epoch [4/10], Step [220/805], Loss: 8.5780\n",
            "Epoch [4/10], Step [230/805], Loss: 8.4776\n",
            "Epoch [4/10], Step [240/805], Loss: 11.7681\n",
            "Epoch [4/10], Step [250/805], Loss: 8.8901\n",
            "Epoch [4/10], Step [260/805], Loss: 11.1275\n",
            "Epoch [4/10], Step [270/805], Loss: 4.8311\n",
            "Epoch [4/10], Step [280/805], Loss: 8.7089\n",
            "Epoch [4/10], Step [290/805], Loss: 10.1074\n",
            "Epoch [4/10], Step [300/805], Loss: 6.8047\n",
            "Epoch [4/10], Step [310/805], Loss: 6.3571\n",
            "Epoch [4/10], Step [320/805], Loss: 12.5466\n",
            "Epoch [4/10], Step [330/805], Loss: 6.8297\n",
            "Epoch [4/10], Step [340/805], Loss: 8.4766\n",
            "Epoch [4/10], Step [350/805], Loss: 11.0200\n",
            "Epoch [4/10], Step [360/805], Loss: 6.9188\n",
            "Epoch [4/10], Step [370/805], Loss: 10.1385\n",
            "Epoch [4/10], Step [380/805], Loss: 6.6846\n",
            "Epoch [4/10], Step [390/805], Loss: 7.7229\n",
            "Epoch [4/10], Step [400/805], Loss: 5.5036\n",
            "Epoch [4/10], Step [410/805], Loss: 10.1478\n",
            "Epoch [4/10], Step [420/805], Loss: 7.5007\n",
            "Epoch [4/10], Step [430/805], Loss: 8.5796\n",
            "Epoch [4/10], Step [440/805], Loss: 7.7833\n",
            "Epoch [4/10], Step [450/805], Loss: 7.7345\n",
            "Epoch [4/10], Step [460/805], Loss: 6.4089\n",
            "Epoch [4/10], Step [470/805], Loss: 10.6694\n",
            "Epoch [4/10], Step [480/805], Loss: 7.6778\n",
            "Epoch [4/10], Step [490/805], Loss: 7.1389\n",
            "Epoch [4/10], Step [500/805], Loss: 7.2271\n",
            "Epoch [4/10], Step [510/805], Loss: 10.7889\n",
            "Epoch [4/10], Step [520/805], Loss: 12.8310\n",
            "Epoch [4/10], Step [530/805], Loss: 6.4378\n",
            "Epoch [4/10], Step [540/805], Loss: 10.8564\n",
            "Epoch [4/10], Step [550/805], Loss: 9.5597\n",
            "Epoch [4/10], Step [560/805], Loss: 6.9207\n",
            "Epoch [4/10], Step [570/805], Loss: 7.9548\n",
            "Epoch [4/10], Step [580/805], Loss: 6.6552\n",
            "Epoch [4/10], Step [590/805], Loss: 8.6635\n",
            "Epoch [4/10], Step [600/805], Loss: 9.7570\n",
            "Epoch [4/10], Step [610/805], Loss: 9.1311\n",
            "Epoch [4/10], Step [620/805], Loss: 10.1235\n",
            "Epoch [4/10], Step [630/805], Loss: 8.2897\n",
            "Epoch [4/10], Step [640/805], Loss: 7.7693\n",
            "Epoch [4/10], Step [650/805], Loss: 8.8244\n",
            "Epoch [4/10], Step [660/805], Loss: 10.8692\n",
            "Epoch [4/10], Step [670/805], Loss: 5.8828\n",
            "Epoch [4/10], Step [680/805], Loss: 8.7986\n",
            "Epoch [4/10], Step [690/805], Loss: 9.6093\n",
            "Epoch [4/10], Step [700/805], Loss: 9.5583\n",
            "Epoch [4/10], Step [710/805], Loss: 10.2517\n",
            "Epoch [4/10], Step [720/805], Loss: 5.4653\n",
            "Epoch [4/10], Step [730/805], Loss: 8.4898\n",
            "Epoch [4/10], Step [740/805], Loss: 7.4043\n",
            "Epoch [4/10], Step [750/805], Loss: 7.0894\n",
            "Epoch [4/10], Step [760/805], Loss: 10.0519\n",
            "Epoch [4/10], Step [770/805], Loss: 13.4742\n",
            "Epoch [4/10], Step [780/805], Loss: 7.3509\n",
            "Epoch [4/10], Step [790/805], Loss: 5.8674\n",
            "Epoch [4/10], Step [800/805], Loss: 11.6207\n",
            "Epoch [4/10], Average Loss: 8.5114\n",
            "Epoch [5/10], Step [0/805], Loss: 5.2543\n",
            "Epoch [5/10], Step [10/805], Loss: 14.2427\n",
            "Epoch [5/10], Step [20/805], Loss: 7.6074\n",
            "Epoch [5/10], Step [30/805], Loss: 11.8476\n",
            "Epoch [5/10], Step [40/805], Loss: 6.6402\n",
            "Epoch [5/10], Step [50/805], Loss: 5.7062\n",
            "Epoch [5/10], Step [60/805], Loss: 7.4742\n",
            "Epoch [5/10], Step [70/805], Loss: 7.9337\n",
            "Epoch [5/10], Step [80/805], Loss: 9.0520\n",
            "Epoch [5/10], Step [90/805], Loss: 6.5757\n",
            "Epoch [5/10], Step [100/805], Loss: 8.9433\n",
            "Epoch [5/10], Step [110/805], Loss: 9.3012\n",
            "Epoch [5/10], Step [120/805], Loss: 8.3731\n",
            "Epoch [5/10], Step [130/805], Loss: 7.3462\n",
            "Epoch [5/10], Step [140/805], Loss: 6.1957\n",
            "Epoch [5/10], Step [150/805], Loss: 5.1971\n",
            "Epoch [5/10], Step [160/805], Loss: 4.4201\n",
            "Epoch [5/10], Step [170/805], Loss: 7.0723\n",
            "Epoch [5/10], Step [180/805], Loss: 9.5731\n",
            "Epoch [5/10], Step [190/805], Loss: 8.4737\n",
            "Epoch [5/10], Step [200/805], Loss: 10.4034\n",
            "Epoch [5/10], Step [210/805], Loss: 6.6465\n",
            "Epoch [5/10], Step [220/805], Loss: 7.6641\n",
            "Epoch [5/10], Step [230/805], Loss: 10.6959\n",
            "Epoch [5/10], Step [240/805], Loss: 8.8719\n",
            "Epoch [5/10], Step [250/805], Loss: 10.6499\n",
            "Epoch [5/10], Step [260/805], Loss: 10.5693\n",
            "Epoch [5/10], Step [270/805], Loss: 9.4068\n",
            "Epoch [5/10], Step [280/805], Loss: 8.9365\n",
            "Epoch [5/10], Step [290/805], Loss: 6.9338\n",
            "Epoch [5/10], Step [300/805], Loss: 9.1951\n",
            "Epoch [5/10], Step [310/805], Loss: 6.8974\n",
            "Epoch [5/10], Step [320/805], Loss: 8.3495\n",
            "Epoch [5/10], Step [330/805], Loss: 9.7543\n",
            "Epoch [5/10], Step [340/805], Loss: 7.7518\n",
            "Epoch [5/10], Step [350/805], Loss: 11.1686\n",
            "Epoch [5/10], Step [360/805], Loss: 7.8288\n",
            "Epoch [5/10], Step [370/805], Loss: 8.5395\n",
            "Epoch [5/10], Step [380/805], Loss: 5.8633\n",
            "Epoch [5/10], Step [390/805], Loss: 6.7619\n",
            "Epoch [5/10], Step [400/805], Loss: 9.7498\n",
            "Epoch [5/10], Step [410/805], Loss: 4.1689\n",
            "Epoch [5/10], Step [420/805], Loss: 10.0527\n",
            "Epoch [5/10], Step [430/805], Loss: 9.6730\n",
            "Epoch [5/10], Step [440/805], Loss: 7.3221\n",
            "Epoch [5/10], Step [450/805], Loss: 9.5556\n",
            "Epoch [5/10], Step [460/805], Loss: 8.2108\n",
            "Epoch [5/10], Step [470/805], Loss: 4.8880\n",
            "Epoch [5/10], Step [480/805], Loss: 8.5404\n",
            "Epoch [5/10], Step [490/805], Loss: 11.1414\n",
            "Epoch [5/10], Step [500/805], Loss: 8.5800\n",
            "Epoch [5/10], Step [510/805], Loss: 12.6381\n",
            "Epoch [5/10], Step [520/805], Loss: 9.9300\n",
            "Epoch [5/10], Step [530/805], Loss: 7.1685\n",
            "Epoch [5/10], Step [540/805], Loss: 9.2019\n",
            "Epoch [5/10], Step [550/805], Loss: 10.6514\n",
            "Epoch [5/10], Step [560/805], Loss: 11.4529\n",
            "Epoch [5/10], Step [570/805], Loss: 6.9224\n",
            "Epoch [5/10], Step [580/805], Loss: 9.1576\n",
            "Epoch [5/10], Step [590/805], Loss: 9.1583\n",
            "Epoch [5/10], Step [600/805], Loss: 6.6490\n",
            "Epoch [5/10], Step [610/805], Loss: 7.0856\n",
            "Epoch [5/10], Step [620/805], Loss: 4.5562\n",
            "Epoch [5/10], Step [630/805], Loss: 7.6867\n",
            "Epoch [5/10], Step [640/805], Loss: 6.8219\n",
            "Epoch [5/10], Step [650/805], Loss: 5.6428\n",
            "Epoch [5/10], Step [660/805], Loss: 10.8562\n",
            "Epoch [5/10], Step [670/805], Loss: 6.7689\n",
            "Epoch [5/10], Step [680/805], Loss: 8.4576\n",
            "Epoch [5/10], Step [690/805], Loss: 8.0487\n",
            "Epoch [5/10], Step [700/805], Loss: 10.8827\n",
            "Epoch [5/10], Step [710/805], Loss: 5.1602\n",
            "Epoch [5/10], Step [720/805], Loss: 6.4280\n",
            "Epoch [5/10], Step [730/805], Loss: 9.7219\n",
            "Epoch [5/10], Step [740/805], Loss: 10.8767\n",
            "Epoch [5/10], Step [750/805], Loss: 10.3165\n",
            "Epoch [5/10], Step [760/805], Loss: 10.5038\n",
            "Epoch [5/10], Step [770/805], Loss: 9.2738\n",
            "Epoch [5/10], Step [780/805], Loss: 8.0491\n",
            "Epoch [5/10], Step [790/805], Loss: 6.8215\n",
            "Epoch [5/10], Step [800/805], Loss: 6.5991\n",
            "Epoch [5/10], Average Loss: 8.5050\n",
            "Epoch [6/10], Step [0/805], Loss: 5.7975\n",
            "Epoch [6/10], Step [10/805], Loss: 11.0817\n",
            "Epoch [6/10], Step [20/805], Loss: 6.0248\n",
            "Epoch [6/10], Step [30/805], Loss: 7.8508\n",
            "Epoch [6/10], Step [40/805], Loss: 7.8985\n",
            "Epoch [6/10], Step [50/805], Loss: 10.8679\n",
            "Epoch [6/10], Step [60/805], Loss: 7.5560\n",
            "Epoch [6/10], Step [70/805], Loss: 13.2477\n",
            "Epoch [6/10], Step [80/805], Loss: 5.5555\n",
            "Epoch [6/10], Step [90/805], Loss: 13.0924\n",
            "Epoch [6/10], Step [100/805], Loss: 6.6082\n",
            "Epoch [6/10], Step [110/805], Loss: 8.1071\n",
            "Epoch [6/10], Step [120/805], Loss: 9.2635\n",
            "Epoch [6/10], Step [130/805], Loss: 9.1871\n",
            "Epoch [6/10], Step [140/805], Loss: 7.1476\n",
            "Epoch [6/10], Step [150/805], Loss: 7.1605\n",
            "Epoch [6/10], Step [160/805], Loss: 12.0782\n",
            "Epoch [6/10], Step [170/805], Loss: 5.3705\n",
            "Epoch [6/10], Step [180/805], Loss: 8.9778\n",
            "Epoch [6/10], Step [190/805], Loss: 8.1419\n",
            "Epoch [6/10], Step [200/805], Loss: 8.2078\n",
            "Epoch [6/10], Step [210/805], Loss: 11.2152\n",
            "Epoch [6/10], Step [220/805], Loss: 8.1757\n",
            "Epoch [6/10], Step [230/805], Loss: 7.5737\n",
            "Epoch [6/10], Step [240/805], Loss: 9.2443\n",
            "Epoch [6/10], Step [250/805], Loss: 8.4510\n",
            "Epoch [6/10], Step [260/805], Loss: 10.2070\n",
            "Epoch [6/10], Step [270/805], Loss: 8.0103\n",
            "Epoch [6/10], Step [280/805], Loss: 8.5135\n",
            "Epoch [6/10], Step [290/805], Loss: 6.4558\n",
            "Epoch [6/10], Step [300/805], Loss: 5.7837\n",
            "Epoch [6/10], Step [310/805], Loss: 6.8803\n",
            "Epoch [6/10], Step [320/805], Loss: 7.2873\n",
            "Epoch [6/10], Step [330/805], Loss: 12.1606\n",
            "Epoch [6/10], Step [340/805], Loss: 8.7849\n",
            "Epoch [6/10], Step [350/805], Loss: 9.0657\n",
            "Epoch [6/10], Step [360/805], Loss: 10.1173\n",
            "Epoch [6/10], Step [370/805], Loss: 11.7497\n",
            "Epoch [6/10], Step [380/805], Loss: 7.6871\n",
            "Epoch [6/10], Step [390/805], Loss: 7.5062\n",
            "Epoch [6/10], Step [400/805], Loss: 7.1083\n",
            "Epoch [6/10], Step [410/805], Loss: 10.8037\n",
            "Epoch [6/10], Step [420/805], Loss: 7.5902\n",
            "Epoch [6/10], Step [430/805], Loss: 7.7061\n",
            "Epoch [6/10], Step [440/805], Loss: 8.6817\n",
            "Epoch [6/10], Step [450/805], Loss: 8.3172\n",
            "Epoch [6/10], Step [460/805], Loss: 7.4748\n",
            "Epoch [6/10], Step [470/805], Loss: 5.1869\n",
            "Epoch [6/10], Step [480/805], Loss: 9.9255\n",
            "Epoch [6/10], Step [490/805], Loss: 6.0952\n",
            "Epoch [6/10], Step [500/805], Loss: 9.8884\n",
            "Epoch [6/10], Step [510/805], Loss: 12.2594\n",
            "Epoch [6/10], Step [520/805], Loss: 8.5261\n",
            "Epoch [6/10], Step [530/805], Loss: 8.6404\n",
            "Epoch [6/10], Step [540/805], Loss: 5.3338\n",
            "Epoch [6/10], Step [550/805], Loss: 6.9717\n",
            "Epoch [6/10], Step [560/805], Loss: 11.0249\n",
            "Epoch [6/10], Step [570/805], Loss: 3.8747\n",
            "Epoch [6/10], Step [580/805], Loss: 7.7868\n",
            "Epoch [6/10], Step [590/805], Loss: 10.0156\n",
            "Epoch [6/10], Step [600/805], Loss: 8.3714\n",
            "Epoch [6/10], Step [610/805], Loss: 7.8436\n",
            "Epoch [6/10], Step [620/805], Loss: 7.2686\n",
            "Epoch [6/10], Step [630/805], Loss: 6.1753\n",
            "Epoch [6/10], Step [640/805], Loss: 9.0250\n",
            "Epoch [6/10], Step [650/805], Loss: 9.5640\n",
            "Epoch [6/10], Step [660/805], Loss: 6.9444\n",
            "Epoch [6/10], Step [670/805], Loss: 6.7640\n",
            "Epoch [6/10], Step [680/805], Loss: 7.7373\n",
            "Epoch [6/10], Step [690/805], Loss: 6.5828\n",
            "Epoch [6/10], Step [700/805], Loss: 7.5784\n",
            "Epoch [6/10], Step [710/805], Loss: 10.5846\n",
            "Epoch [6/10], Step [720/805], Loss: 10.1197\n",
            "Epoch [6/10], Step [730/805], Loss: 9.6519\n",
            "Epoch [6/10], Step [740/805], Loss: 8.8067\n",
            "Epoch [6/10], Step [750/805], Loss: 9.2969\n",
            "Epoch [6/10], Step [760/805], Loss: 11.1871\n",
            "Epoch [6/10], Step [770/805], Loss: 8.3254\n",
            "Epoch [6/10], Step [780/805], Loss: 7.2058\n",
            "Epoch [6/10], Step [790/805], Loss: 5.1836\n",
            "Epoch [6/10], Step [800/805], Loss: 6.5122\n",
            "Epoch [6/10], Average Loss: 8.4949\n",
            "Epoch [7/10], Step [0/805], Loss: 10.9463\n",
            "Epoch [7/10], Step [10/805], Loss: 8.9524\n",
            "Epoch [7/10], Step [20/805], Loss: 9.0279\n",
            "Epoch [7/10], Step [30/805], Loss: 7.8796\n",
            "Epoch [7/10], Step [40/805], Loss: 7.6211\n",
            "Epoch [7/10], Step [50/805], Loss: 6.3259\n",
            "Epoch [7/10], Step [60/805], Loss: 8.8585\n",
            "Epoch [7/10], Step [70/805], Loss: 6.9029\n",
            "Epoch [7/10], Step [80/805], Loss: 8.7505\n",
            "Epoch [7/10], Step [90/805], Loss: 7.7448\n",
            "Epoch [7/10], Step [100/805], Loss: 6.5719\n",
            "Epoch [7/10], Step [110/805], Loss: 9.8828\n",
            "Epoch [7/10], Step [120/805], Loss: 7.6646\n",
            "Epoch [7/10], Step [130/805], Loss: 6.2864\n",
            "Epoch [7/10], Step [140/805], Loss: 7.1968\n",
            "Epoch [7/10], Step [150/805], Loss: 9.9635\n",
            "Epoch [7/10], Step [160/805], Loss: 12.0461\n",
            "Epoch [7/10], Step [170/805], Loss: 5.8171\n",
            "Epoch [7/10], Step [180/805], Loss: 5.4798\n",
            "Epoch [7/10], Step [190/805], Loss: 7.2506\n",
            "Epoch [7/10], Step [200/805], Loss: 6.1527\n",
            "Epoch [7/10], Step [210/805], Loss: 8.5366\n",
            "Epoch [7/10], Step [220/805], Loss: 7.4207\n",
            "Epoch [7/10], Step [230/805], Loss: 6.0955\n",
            "Epoch [7/10], Step [240/805], Loss: 9.6517\n",
            "Epoch [7/10], Step [250/805], Loss: 11.0712\n",
            "Epoch [7/10], Step [260/805], Loss: 7.2449\n",
            "Epoch [7/10], Step [270/805], Loss: 5.7601\n",
            "Epoch [7/10], Step [280/805], Loss: 11.4197\n",
            "Epoch [7/10], Step [290/805], Loss: 4.5353\n",
            "Epoch [7/10], Step [300/805], Loss: 6.6779\n",
            "Epoch [7/10], Step [310/805], Loss: 9.3977\n",
            "Epoch [7/10], Step [320/805], Loss: 10.2482\n",
            "Epoch [7/10], Step [330/805], Loss: 7.5673\n",
            "Epoch [7/10], Step [340/805], Loss: 9.8000\n",
            "Epoch [7/10], Step [350/805], Loss: 9.6654\n",
            "Epoch [7/10], Step [360/805], Loss: 11.1723\n",
            "Epoch [7/10], Step [370/805], Loss: 14.7234\n",
            "Epoch [7/10], Step [380/805], Loss: 9.9118\n",
            "Epoch [7/10], Step [390/805], Loss: 8.9150\n",
            "Epoch [7/10], Step [400/805], Loss: 6.3563\n",
            "Epoch [7/10], Step [410/805], Loss: 13.4808\n",
            "Epoch [7/10], Step [420/805], Loss: 6.0117\n",
            "Epoch [7/10], Step [430/805], Loss: 7.9943\n",
            "Epoch [7/10], Step [440/805], Loss: 5.3416\n",
            "Epoch [7/10], Step [450/805], Loss: 5.3327\n",
            "Epoch [7/10], Step [460/805], Loss: 11.4956\n",
            "Epoch [7/10], Step [470/805], Loss: 7.8019\n",
            "Epoch [7/10], Step [480/805], Loss: 6.6696\n",
            "Epoch [7/10], Step [490/805], Loss: 9.2544\n",
            "Epoch [7/10], Step [500/805], Loss: 11.2517\n",
            "Epoch [7/10], Step [510/805], Loss: 8.9893\n",
            "Epoch [7/10], Step [520/805], Loss: 7.5487\n",
            "Epoch [7/10], Step [530/805], Loss: 7.5630\n",
            "Epoch [7/10], Step [540/805], Loss: 7.4245\n",
            "Epoch [7/10], Step [550/805], Loss: 8.1716\n",
            "Epoch [7/10], Step [560/805], Loss: 8.6913\n",
            "Epoch [7/10], Step [570/805], Loss: 10.7561\n",
            "Epoch [7/10], Step [580/805], Loss: 12.0508\n",
            "Epoch [7/10], Step [590/805], Loss: 11.7513\n",
            "Epoch [7/10], Step [600/805], Loss: 9.3772\n",
            "Epoch [7/10], Step [610/805], Loss: 11.1839\n",
            "Epoch [7/10], Step [620/805], Loss: 10.5030\n",
            "Epoch [7/10], Step [630/805], Loss: 10.8591\n",
            "Epoch [7/10], Step [640/805], Loss: 15.6018\n",
            "Epoch [7/10], Step [650/805], Loss: 10.4644\n",
            "Epoch [7/10], Step [660/805], Loss: 7.5270\n",
            "Epoch [7/10], Step [670/805], Loss: 6.6011\n",
            "Epoch [7/10], Step [680/805], Loss: 10.4793\n",
            "Epoch [7/10], Step [690/805], Loss: 6.6935\n",
            "Epoch [7/10], Step [700/805], Loss: 11.8426\n",
            "Epoch [7/10], Step [710/805], Loss: 7.6579\n",
            "Epoch [7/10], Step [720/805], Loss: 8.8730\n",
            "Epoch [7/10], Step [730/805], Loss: 10.8626\n",
            "Epoch [7/10], Step [740/805], Loss: 5.6599\n",
            "Epoch [7/10], Step [750/805], Loss: 5.4106\n",
            "Epoch [7/10], Step [760/805], Loss: 9.1776\n",
            "Epoch [7/10], Step [770/805], Loss: 8.1619\n",
            "Epoch [7/10], Step [780/805], Loss: 5.6628\n",
            "Epoch [7/10], Step [790/805], Loss: 9.9417\n",
            "Epoch [7/10], Step [800/805], Loss: 6.3366\n",
            "Epoch [7/10], Average Loss: 8.4833\n",
            "Epoch [8/10], Step [0/805], Loss: 4.9074\n",
            "Epoch [8/10], Step [10/805], Loss: 9.8802\n",
            "Epoch [8/10], Step [20/805], Loss: 9.3112\n",
            "Epoch [8/10], Step [30/805], Loss: 8.4472\n",
            "Epoch [8/10], Step [40/805], Loss: 11.4536\n",
            "Epoch [8/10], Step [50/805], Loss: 6.3329\n",
            "Epoch [8/10], Step [60/805], Loss: 7.0623\n",
            "Epoch [8/10], Step [70/805], Loss: 13.0461\n",
            "Epoch [8/10], Step [80/805], Loss: 8.1770\n",
            "Epoch [8/10], Step [90/805], Loss: 5.9192\n",
            "Epoch [8/10], Step [100/805], Loss: 8.4905\n",
            "Epoch [8/10], Step [110/805], Loss: 5.4281\n",
            "Epoch [8/10], Step [120/805], Loss: 9.9495\n",
            "Epoch [8/10], Step [130/805], Loss: 5.0235\n",
            "Epoch [8/10], Step [140/805], Loss: 5.4118\n",
            "Epoch [8/10], Step [150/805], Loss: 7.9616\n",
            "Epoch [8/10], Step [160/805], Loss: 6.6870\n",
            "Epoch [8/10], Step [170/805], Loss: 7.5127\n",
            "Epoch [8/10], Step [180/805], Loss: 5.7988\n",
            "Epoch [8/10], Step [190/805], Loss: 10.8677\n",
            "Epoch [8/10], Step [200/805], Loss: 9.5011\n",
            "Epoch [8/10], Step [210/805], Loss: 8.9164\n",
            "Epoch [8/10], Step [220/805], Loss: 8.0767\n",
            "Epoch [8/10], Step [230/805], Loss: 7.8995\n",
            "Epoch [8/10], Step [240/805], Loss: 6.4940\n",
            "Epoch [8/10], Step [250/805], Loss: 10.2843\n",
            "Epoch [8/10], Step [260/805], Loss: 7.3462\n",
            "Epoch [8/10], Step [270/805], Loss: 11.1847\n",
            "Epoch [8/10], Step [280/805], Loss: 6.2331\n",
            "Epoch [8/10], Step [290/805], Loss: 8.4529\n",
            "Epoch [8/10], Step [300/805], Loss: 5.7872\n",
            "Epoch [8/10], Step [310/805], Loss: 6.4795\n",
            "Epoch [8/10], Step [320/805], Loss: 10.4975\n",
            "Epoch [8/10], Step [330/805], Loss: 8.3019\n",
            "Epoch [8/10], Step [340/805], Loss: 8.9055\n",
            "Epoch [8/10], Step [350/805], Loss: 10.1609\n",
            "Epoch [8/10], Step [360/805], Loss: 14.4739\n",
            "Epoch [8/10], Step [370/805], Loss: 5.7652\n",
            "Epoch [8/10], Step [380/805], Loss: 6.8372\n",
            "Epoch [8/10], Step [390/805], Loss: 7.0558\n",
            "Epoch [8/10], Step [400/805], Loss: 9.2623\n",
            "Epoch [8/10], Step [410/805], Loss: 7.3083\n",
            "Epoch [8/10], Step [420/805], Loss: 7.2129\n",
            "Epoch [8/10], Step [430/805], Loss: 8.8328\n",
            "Epoch [8/10], Step [440/805], Loss: 8.0972\n",
            "Epoch [8/10], Step [450/805], Loss: 11.2288\n",
            "Epoch [8/10], Step [460/805], Loss: 6.5940\n",
            "Epoch [8/10], Step [470/805], Loss: 7.8246\n",
            "Epoch [8/10], Step [480/805], Loss: 8.7207\n",
            "Epoch [8/10], Step [490/805], Loss: 7.8292\n",
            "Epoch [8/10], Step [500/805], Loss: 7.5591\n",
            "Epoch [8/10], Step [510/805], Loss: 6.3290\n",
            "Epoch [8/10], Step [520/805], Loss: 7.3547\n",
            "Epoch [8/10], Step [530/805], Loss: 6.5108\n",
            "Epoch [8/10], Step [540/805], Loss: 7.7820\n",
            "Epoch [8/10], Step [550/805], Loss: 5.8349\n",
            "Epoch [8/10], Step [560/805], Loss: 7.7500\n",
            "Epoch [8/10], Step [570/805], Loss: 5.0383\n",
            "Epoch [8/10], Step [580/805], Loss: 12.3093\n",
            "Epoch [8/10], Step [590/805], Loss: 8.7614\n",
            "Epoch [8/10], Step [600/805], Loss: 14.1154\n",
            "Epoch [8/10], Step [610/805], Loss: 9.3620\n",
            "Epoch [8/10], Step [620/805], Loss: 8.3519\n",
            "Epoch [8/10], Step [630/805], Loss: 5.8682\n",
            "Epoch [8/10], Step [640/805], Loss: 9.5344\n",
            "Epoch [8/10], Step [650/805], Loss: 6.1776\n",
            "Epoch [8/10], Step [660/805], Loss: 4.3808\n",
            "Epoch [8/10], Step [670/805], Loss: 5.0119\n",
            "Epoch [8/10], Step [680/805], Loss: 9.7230\n",
            "Epoch [8/10], Step [690/805], Loss: 9.6231\n",
            "Epoch [8/10], Step [700/805], Loss: 7.1856\n",
            "Epoch [8/10], Step [710/805], Loss: 5.4581\n",
            "Epoch [8/10], Step [720/805], Loss: 7.2451\n",
            "Epoch [8/10], Step [730/805], Loss: 6.9027\n",
            "Epoch [8/10], Step [740/805], Loss: 6.7551\n",
            "Epoch [8/10], Step [750/805], Loss: 8.5009\n",
            "Epoch [8/10], Step [760/805], Loss: 7.3851\n",
            "Epoch [8/10], Step [770/805], Loss: 6.7139\n",
            "Epoch [8/10], Step [780/805], Loss: 9.9249\n",
            "Epoch [8/10], Step [790/805], Loss: 9.6134\n",
            "Epoch [8/10], Step [800/805], Loss: 7.1444\n",
            "Epoch [8/10], Average Loss: 8.4704\n",
            "Epoch [9/10], Step [0/805], Loss: 9.0592\n",
            "Epoch [9/10], Step [10/805], Loss: 7.4650\n",
            "Epoch [9/10], Step [20/805], Loss: 8.8607\n",
            "Epoch [9/10], Step [30/805], Loss: 9.9359\n",
            "Epoch [9/10], Step [40/805], Loss: 6.9639\n",
            "Epoch [9/10], Step [50/805], Loss: 11.3418\n",
            "Epoch [9/10], Step [60/805], Loss: 6.6611\n",
            "Epoch [9/10], Step [70/805], Loss: 7.1093\n",
            "Epoch [9/10], Step [80/805], Loss: 8.0131\n",
            "Epoch [9/10], Step [90/805], Loss: 6.5323\n",
            "Epoch [9/10], Step [100/805], Loss: 5.9478\n",
            "Epoch [9/10], Step [110/805], Loss: 7.9514\n",
            "Epoch [9/10], Step [120/805], Loss: 6.5366\n",
            "Epoch [9/10], Step [130/805], Loss: 11.9889\n",
            "Epoch [9/10], Step [140/805], Loss: 6.1124\n",
            "Epoch [9/10], Step [150/805], Loss: 10.2669\n",
            "Epoch [9/10], Step [160/805], Loss: 11.0035\n",
            "Epoch [9/10], Step [170/805], Loss: 6.4794\n",
            "Epoch [9/10], Step [180/805], Loss: 5.4049\n",
            "Epoch [9/10], Step [190/805], Loss: 7.8450\n",
            "Epoch [9/10], Step [200/805], Loss: 4.5829\n",
            "Epoch [9/10], Step [210/805], Loss: 9.2012\n",
            "Epoch [9/10], Step [220/805], Loss: 9.4012\n",
            "Epoch [9/10], Step [230/805], Loss: 9.0774\n",
            "Epoch [9/10], Step [240/805], Loss: 4.9664\n",
            "Epoch [9/10], Step [250/805], Loss: 9.5013\n",
            "Epoch [9/10], Step [260/805], Loss: 8.4443\n",
            "Epoch [9/10], Step [270/805], Loss: 7.9684\n",
            "Epoch [9/10], Step [280/805], Loss: 6.6404\n",
            "Epoch [9/10], Step [290/805], Loss: 11.8389\n",
            "Epoch [9/10], Step [300/805], Loss: 8.3008\n",
            "Epoch [9/10], Step [310/805], Loss: 5.0091\n",
            "Epoch [9/10], Step [320/805], Loss: 7.4899\n",
            "Epoch [9/10], Step [330/805], Loss: 9.3261\n",
            "Epoch [9/10], Step [340/805], Loss: 8.0714\n",
            "Epoch [9/10], Step [350/805], Loss: 7.8182\n",
            "Epoch [9/10], Step [360/805], Loss: 8.8282\n",
            "Epoch [9/10], Step [370/805], Loss: 10.3807\n",
            "Epoch [9/10], Step [380/805], Loss: 6.9292\n",
            "Epoch [9/10], Step [390/805], Loss: 5.2641\n",
            "Epoch [9/10], Step [400/805], Loss: 8.2681\n",
            "Epoch [9/10], Step [410/805], Loss: 9.4745\n",
            "Epoch [9/10], Step [420/805], Loss: 6.4085\n",
            "Epoch [9/10], Step [430/805], Loss: 7.8084\n",
            "Epoch [9/10], Step [440/805], Loss: 13.4796\n",
            "Epoch [9/10], Step [450/805], Loss: 8.2148\n",
            "Epoch [9/10], Step [460/805], Loss: 7.6620\n",
            "Epoch [9/10], Step [470/805], Loss: 6.6775\n",
            "Epoch [9/10], Step [480/805], Loss: 10.7818\n",
            "Epoch [9/10], Step [490/805], Loss: 5.3070\n",
            "Epoch [9/10], Step [500/805], Loss: 4.7430\n",
            "Epoch [9/10], Step [510/805], Loss: 11.5324\n",
            "Epoch [9/10], Step [520/805], Loss: 10.0730\n",
            "Epoch [9/10], Step [530/805], Loss: 6.3969\n",
            "Epoch [9/10], Step [540/805], Loss: 9.2199\n",
            "Epoch [9/10], Step [550/805], Loss: 6.3185\n",
            "Epoch [9/10], Step [560/805], Loss: 8.8399\n",
            "Epoch [9/10], Step [570/805], Loss: 7.3408\n",
            "Epoch [9/10], Step [580/805], Loss: 8.1591\n",
            "Epoch [9/10], Step [590/805], Loss: 11.2941\n",
            "Epoch [9/10], Step [600/805], Loss: 8.8019\n",
            "Epoch [9/10], Step [610/805], Loss: 8.3048\n",
            "Epoch [9/10], Step [620/805], Loss: 8.4008\n",
            "Epoch [9/10], Step [630/805], Loss: 7.8391\n",
            "Epoch [9/10], Step [640/805], Loss: 6.6804\n",
            "Epoch [9/10], Step [650/805], Loss: 11.7262\n",
            "Epoch [9/10], Step [660/805], Loss: 5.7119\n",
            "Epoch [9/10], Step [670/805], Loss: 10.5990\n",
            "Epoch [9/10], Step [680/805], Loss: 13.3732\n",
            "Epoch [9/10], Step [690/805], Loss: 4.0825\n",
            "Epoch [9/10], Step [700/805], Loss: 8.0339\n",
            "Epoch [9/10], Step [710/805], Loss: 10.0912\n",
            "Epoch [9/10], Step [720/805], Loss: 8.1402\n",
            "Epoch [9/10], Step [730/805], Loss: 11.8219\n",
            "Epoch [9/10], Step [740/805], Loss: 8.0705\n",
            "Epoch [9/10], Step [750/805], Loss: 14.1202\n",
            "Epoch [9/10], Step [760/805], Loss: 6.7162\n",
            "Epoch [9/10], Step [770/805], Loss: 15.7280\n",
            "Epoch [9/10], Step [780/805], Loss: 6.2169\n",
            "Epoch [9/10], Step [790/805], Loss: 9.4931\n",
            "Epoch [9/10], Step [800/805], Loss: 5.8465\n",
            "Epoch [9/10], Average Loss: 8.4737\n",
            "Epoch [10/10], Step [0/805], Loss: 12.0515\n",
            "Epoch [10/10], Step [10/805], Loss: 9.2641\n",
            "Epoch [10/10], Step [20/805], Loss: 13.5016\n",
            "Epoch [10/10], Step [30/805], Loss: 8.6635\n",
            "Epoch [10/10], Step [40/805], Loss: 9.1979\n",
            "Epoch [10/10], Step [50/805], Loss: 12.0203\n",
            "Epoch [10/10], Step [60/805], Loss: 9.4704\n",
            "Epoch [10/10], Step [70/805], Loss: 5.6591\n",
            "Epoch [10/10], Step [80/805], Loss: 4.9492\n",
            "Epoch [10/10], Step [90/805], Loss: 9.5369\n",
            "Epoch [10/10], Step [100/805], Loss: 7.1174\n",
            "Epoch [10/10], Step [110/805], Loss: 8.6215\n",
            "Epoch [10/10], Step [120/805], Loss: 10.0560\n",
            "Epoch [10/10], Step [130/805], Loss: 6.5792\n",
            "Epoch [10/10], Step [140/805], Loss: 7.7985\n",
            "Epoch [10/10], Step [150/805], Loss: 10.6502\n",
            "Epoch [10/10], Step [160/805], Loss: 5.3868\n",
            "Epoch [10/10], Step [170/805], Loss: 7.3187\n",
            "Epoch [10/10], Step [180/805], Loss: 6.6901\n",
            "Epoch [10/10], Step [190/805], Loss: 7.9966\n",
            "Epoch [10/10], Step [200/805], Loss: 10.0548\n",
            "Epoch [10/10], Step [210/805], Loss: 6.9595\n",
            "Epoch [10/10], Step [220/805], Loss: 16.0605\n",
            "Epoch [10/10], Step [230/805], Loss: 7.4291\n",
            "Epoch [10/10], Step [240/805], Loss: 10.8428\n",
            "Epoch [10/10], Step [250/805], Loss: 6.2386\n",
            "Epoch [10/10], Step [260/805], Loss: 7.6792\n",
            "Epoch [10/10], Step [270/805], Loss: 8.7069\n",
            "Epoch [10/10], Step [280/805], Loss: 8.6295\n",
            "Epoch [10/10], Step [290/805], Loss: 5.5704\n",
            "Epoch [10/10], Step [300/805], Loss: 4.1930\n",
            "Epoch [10/10], Step [310/805], Loss: 8.5044\n",
            "Epoch [10/10], Step [320/805], Loss: 8.1533\n",
            "Epoch [10/10], Step [330/805], Loss: 8.8272\n",
            "Epoch [10/10], Step [340/805], Loss: 7.9030\n",
            "Epoch [10/10], Step [350/805], Loss: 8.4120\n",
            "Epoch [10/10], Step [360/805], Loss: 7.4725\n",
            "Epoch [10/10], Step [370/805], Loss: 8.3058\n",
            "Epoch [10/10], Step [380/805], Loss: 9.3231\n",
            "Epoch [10/10], Step [390/805], Loss: 8.0685\n",
            "Epoch [10/10], Step [400/805], Loss: 11.2690\n",
            "Epoch [10/10], Step [410/805], Loss: 7.3638\n",
            "Epoch [10/10], Step [420/805], Loss: 6.6081\n",
            "Epoch [10/10], Step [430/805], Loss: 6.4458\n",
            "Epoch [10/10], Step [440/805], Loss: 9.2285\n",
            "Epoch [10/10], Step [450/805], Loss: 7.4602\n",
            "Epoch [10/10], Step [460/805], Loss: 12.1316\n",
            "Epoch [10/10], Step [470/805], Loss: 13.1694\n",
            "Epoch [10/10], Step [480/805], Loss: 6.4192\n",
            "Epoch [10/10], Step [490/805], Loss: 11.5506\n",
            "Epoch [10/10], Step [500/805], Loss: 6.7707\n",
            "Epoch [10/10], Step [510/805], Loss: 10.6206\n",
            "Epoch [10/10], Step [520/805], Loss: 7.6356\n",
            "Epoch [10/10], Step [530/805], Loss: 10.7198\n",
            "Epoch [10/10], Step [540/805], Loss: 6.7268\n",
            "Epoch [10/10], Step [550/805], Loss: 9.4556\n",
            "Epoch [10/10], Step [560/805], Loss: 9.6424\n",
            "Epoch [10/10], Step [570/805], Loss: 10.7435\n",
            "Epoch [10/10], Step [580/805], Loss: 8.3905\n",
            "Epoch [10/10], Step [590/805], Loss: 8.0457\n",
            "Epoch [10/10], Step [600/805], Loss: 11.4560\n",
            "Epoch [10/10], Step [610/805], Loss: 7.5625\n",
            "Epoch [10/10], Step [620/805], Loss: 7.1130\n",
            "Epoch [10/10], Step [630/805], Loss: 13.6273\n",
            "Epoch [10/10], Step [640/805], Loss: 6.9423\n",
            "Epoch [10/10], Step [650/805], Loss: 7.1572\n",
            "Epoch [10/10], Step [660/805], Loss: 10.5944\n",
            "Epoch [10/10], Step [670/805], Loss: 8.0329\n",
            "Epoch [10/10], Step [680/805], Loss: 7.6961\n",
            "Epoch [10/10], Step [690/805], Loss: 7.3583\n",
            "Epoch [10/10], Step [700/805], Loss: 6.2056\n",
            "Epoch [10/10], Step [710/805], Loss: 6.6312\n",
            "Epoch [10/10], Step [720/805], Loss: 8.8241\n",
            "Epoch [10/10], Step [730/805], Loss: 6.1411\n",
            "Epoch [10/10], Step [740/805], Loss: 5.7422\n",
            "Epoch [10/10], Step [750/805], Loss: 5.9552\n",
            "Epoch [10/10], Step [760/805], Loss: 6.9096\n",
            "Epoch [10/10], Step [770/805], Loss: 11.9990\n",
            "Epoch [10/10], Step [780/805], Loss: 5.6385\n",
            "Epoch [10/10], Step [790/805], Loss: 11.0187\n",
            "Epoch [10/10], Step [800/805], Loss: 9.3385\n",
            "Epoch [10/10], Average Loss: 8.4654\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "        images, targets = images.to(device), targets.to(device)  # Move both images and targets to device\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = yolo_loss(outputs, targets, S=S, B=B)  # Ensure yolo_loss handles device tensors\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"yolov1_widerface_model.pth\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}